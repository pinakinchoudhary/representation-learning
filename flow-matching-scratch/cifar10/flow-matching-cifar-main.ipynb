{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f375107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d052bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Experiment Configuration ==================\n",
    "EXPERIMENT_NAME = \"baseline\"  # Change this for each experiment\n",
    "RESULTS_DIR = f\"results/{EXPERIMENT_NAME}\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Configuration Parameters ==================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 32  # CIFAR-10 is 32x32\n",
    "channels = 3     # CIFAR-10 is RGB\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "model_save_path = os.path.join(RESULTS_DIR, 'FMmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1365f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "lr = 1e-4\n",
    "# lr = 1e-3\n",
    "\n",
    "# Number of Epochs\n",
    "epochs = 100\n",
    "\n",
    "# Model Size\n",
    "BASE_CHANNELS = 128\n",
    "\n",
    "# Time Embedding Dimension \n",
    "TIME_EMBED_DIM = 128\n",
    "\n",
    "# ODE Solver\n",
    "ODE_METHOD = 'dopri5'  # adaptive Runge-Kutta\n",
    "# ODE_METHOD = 'euler'  # simplest method: Euler\n",
    "# ODE_METHOD = 'rk4' # heun's method\n",
    "\n",
    "# Number of ODE Steps \n",
    "ODE_STEPS = 50  #100\n",
    "\n",
    "# Loss Function: b/w u^target and u^theta\n",
    "LOSS_TYPE = 'mse'  # Mean Squared Error\n",
    "# LOSS_TYPE = 'l1'   # L1 Loss (Mean Absolute Error)\n",
    "# LOSS_TYPE = 'huber'  # Huber Loss\n",
    "\n",
    "# Normalization \n",
    "NORM_TYPE = 'groupnorm'\n",
    "# NORM_TYPE = 'batchnorm'\n",
    "# NORM_TYPE = 'layernorm'\n",
    "\n",
    "# Activation Function\n",
    "ACTIVATION = 'silu'  # SiLU/Swish\n",
    "\n",
    "# Optimizer \n",
    "OPTIMIZER_TYPE = 'adamw'\n",
    "# OPTIMIZER_TYPE = 'sgd'\n",
    "\n",
    "# Save experiment config\n",
    "config = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'lr': lr,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'base_channels': BASE_CHANNELS,\n",
    "    'time_embed_dim': TIME_EMBED_DIM,\n",
    "    'ode_method': ODE_METHOD,\n",
    "    'ode_steps': ODE_STEPS,\n",
    "    'loss_type': LOSS_TYPE,\n",
    "    'norm_type': NORM_TYPE,\n",
    "    'activation': ACTIVATION,\n",
    "    'optimizer_type': OPTIMIZER_TYPE,\n",
    "    'image_size': image_size,\n",
    "    'channels': channels,\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d287714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Data Loading ==================\n",
    "def normalize_img(x):\n",
    "    \"\"\"Normalize image to [-1, 1]\"\"\"\n",
    "    return 2 * x - 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(normalize_img)\n",
    "])\n",
    "\n",
    "# ================== Helper Functions ==================\n",
    "def get_activation():\n",
    "    \"\"\"Return activation function based on config\"\"\"\n",
    "    if ACTIVATION == 'silu':\n",
    "        return F.silu\n",
    "    elif ACTIVATION == 'swiglu':\n",
    "        return SwiGLU()\n",
    "    elif ACTIVATION == 'gelu':\n",
    "        return F.gelu\n",
    "    else:\n",
    "        return F.silu\n",
    "\n",
    "def get_norm_layer(num_channels):\n",
    "    \"\"\"Return normalization layer based on config\"\"\"\n",
    "    if NORM_TYPE == 'groupnorm':\n",
    "        return nn.GroupNorm(min(32, num_channels), num_channels)\n",
    "    elif NORM_TYPE == 'batchnorm':\n",
    "        return nn.BatchNorm2d(num_channels)\n",
    "    elif NORM_TYPE == 'layernorm':\n",
    "        # For LayerNorm with images, normalize over C, H, W\n",
    "        return nn.GroupNorm(1, num_channels)\n",
    "    else:\n",
    "        return nn.GroupNorm(min(32, num_channels), num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model Architecture ==================\n",
    "class ConditionedDoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution module with condition injection\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = get_norm_layer(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels + cond_dim, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = get_norm_layer(out_channels)\n",
    "        self.activation = get_activation()\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        x = self.activation(self.norm1(self.conv1(x)))\n",
    "        cond = cond.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, cond], dim=1)\n",
    "        return self.activation(self.norm2(self.conv2(x)))\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling module\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv = ConditionedDoubleConv(in_channels, out_channels, cond_dim)\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        x = self.maxpool(x)\n",
    "        return self.conv(x, cond)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling module\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = ConditionedDoubleConv(in_channels, out_channels, cond_dim)\n",
    "    \n",
    "    def forward(self, x1, x2, cond):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x, cond)\n",
    "\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"Enhanced UNet for CIFAR-10\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Condition dimensions\n",
    "        self.label_dim = 32\n",
    "        self.cond_dim = TIME_EMBED_DIM + self.label_dim\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, TIME_EMBED_DIM * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(TIME_EMBED_DIM * 2, TIME_EMBED_DIM)\n",
    "        )\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embed = nn.Embedding(num_classes, self.label_dim)\n",
    "        \n",
    "        # Encoder path (deeper for CIFAR-10)\n",
    "        self.inc = ConditionedDoubleConv(channels, BASE_CHANNELS, self.cond_dim)\n",
    "        self.down1 = Down(BASE_CHANNELS, BASE_CHANNELS * 2, self.cond_dim)\n",
    "        self.down2 = Down(BASE_CHANNELS * 2, BASE_CHANNELS * 4, self.cond_dim)\n",
    "        self.down3 = Down(BASE_CHANNELS * 4, BASE_CHANNELS * 8, self.cond_dim)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = Up(BASE_CHANNELS * 8 + BASE_CHANNELS * 4, BASE_CHANNELS * 4, self.cond_dim)\n",
    "        self.up2 = Up(BASE_CHANNELS * 4 + BASE_CHANNELS * 2, BASE_CHANNELS * 2, self.cond_dim)\n",
    "        self.up3 = Up(BASE_CHANNELS * 2 + BASE_CHANNELS, BASE_CHANNELS, self.cond_dim)\n",
    "        self.outc = nn.Conv2d(BASE_CHANNELS, channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, t, labels):\n",
    "        # Condition encoding\n",
    "        t_emb = self.time_embed(t.view(-1, 1))\n",
    "        lbl_emb = self.label_embed(labels)\n",
    "        cond = torch.cat([t_emb, lbl_emb], dim=1)\n",
    "        cond = cond.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.inc(x, cond)\n",
    "        x2 = self.down1(x1, cond)\n",
    "        x3 = self.down2(x2, cond)\n",
    "        x4 = self.down3(x3, cond)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x4, x3, cond)\n",
    "        x = self.up2(x, x2, cond)\n",
    "        x = self.up3(x, x1, cond)\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Training and Generation ==================\n",
    "model = ConditionalUNet().to(device)\n",
    "\n",
    "# Initialize optimizer based on config\n",
    "if OPTIMIZER_TYPE == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "elif OPTIMIZER_TYPE == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "elif OPTIMIZER_TYPE == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def compute_loss(pred, target):\n",
    "    \"\"\"Compute loss based on config\"\"\"\n",
    "    if LOSS_TYPE == 'mse':\n",
    "        return F.mse_loss(pred, target)\n",
    "    elif LOSS_TYPE == 'l1':\n",
    "        return F.l1_loss(pred, target)\n",
    "    elif LOSS_TYPE == 'huber':\n",
    "        return F.huber_loss(pred, target)\n",
    "    else:\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_label(label, num_samples=16):\n",
    "    \"\"\"Generate samples with specified label\"\"\"\n",
    "    current_model_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    x0 = torch.randn(num_samples, channels, image_size, image_size, device=device)\n",
    "    labels = torch.full((num_samples,), label, device=device, dtype=torch.long)\n",
    "    \n",
    "    def ode_func(t: torch.Tensor, x: torch.Tensor):\n",
    "        t_expanded = t.expand(x.size(0))\n",
    "        vt = model(x, t_expanded, labels)\n",
    "        return vt\n",
    "    \n",
    "    # Use different time points based on ODE method\n",
    "    if ODE_METHOD in ['euler', 'rk4', 'midpoint']:\n",
    "        t_eval = torch.linspace(0.0, 1.0, ODE_STEPS + 1, device=device)\n",
    "    else:\n",
    "        t_eval = torch.tensor([0.0, 1.0], device=device)\n",
    "    \n",
    "    generated = odeint(\n",
    "        ode_func,\n",
    "        x0,\n",
    "        t_eval,\n",
    "        rtol=1e-5,\n",
    "        atol=1e-5,\n",
    "        method=ODE_METHOD\n",
    "    )\n",
    "    \n",
    "    model.train(current_model_state)\n",
    "    \n",
    "    images = (generated[-1].clamp(-1, 1) + 1) / 2\n",
    "    return images.cpu()\n",
    "\n",
    "\n",
    "def visualize_train(epoch):\n",
    "    \"\"\"Generate visualization grid\"\"\"\n",
    "    print(f\"Generating visualization for epoch {epoch}...\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    \n",
    "    for label in range(num_classes):\n",
    "        generated_images = generate_with_label(label=label, num_samples=10)\n",
    "        \n",
    "        for i in range(10):\n",
    "            ax = plt.subplot(10, num_classes, (i * num_classes) + label + 1)\n",
    "            # Convert CHW to HWC for display\n",
    "            img = generated_images[i].permute(1, 2, 0).numpy()\n",
    "            plt.imshow(img, vmin=0, vmax=1)\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title(str(label), fontsize=14, pad=5)\n",
    "    \n",
    "    plt.suptitle(f\"Generated Samples - Epoch {epoch}\", fontsize=18, y=0.98)\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f\"epoch{epoch}.jpg\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_final_samples(num_sample=5):\n",
    "    \"\"\"Generate final sample grids\"\"\"\n",
    "    print(f\"Generating {num_sample} final sample grids...\")\n",
    "    \n",
    "    for k in range(num_sample):\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        \n",
    "        print(f\"Generating grid {k + 1}/{num_sample}...\")\n",
    "        for label in tqdm(range(num_classes), desc=f\"Grid {k+1}\"):\n",
    "            generated_images = generate_with_label(label=label, num_samples=10)\n",
    "            \n",
    "            for i in range(10):\n",
    "                ax = plt.subplot(10, num_classes, (i * num_classes) + label + 1)\n",
    "                img = generated_images[i].permute(1, 2, 0).numpy()\n",
    "                plt.imshow(img, vmin=0, vmax=1)\n",
    "                ax.axis('off')\n",
    "                if i == 0:\n",
    "                    ax.set_title(str(label), fontsize=14, pad=5)\n",
    "        \n",
    "        plt.suptitle(f\"Final Generated Samples (Grid {k + 1})\", fontsize=18, y=0.98)\n",
    "        save_path = os.path.join(RESULTS_DIR, f\"generated_grid{k + 1}.jpg\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=100):\n",
    "    \"\"\"Training loop with logging\"\"\"\n",
    "    print(f\"Starting training for experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "    \n",
    "    global train_loader\n",
    "    training_log = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            noise = torch.randn_like(images)\n",
    "            t = torch.rand(images.size(0), device=device)\n",
    "            xt = (1 - t.view(-1, 1, 1, 1)) * noise + t.view(-1, 1, 1, 1) * images\n",
    "            \n",
    "            vt_pred = model(xt, t, labels)\n",
    "            loss = compute_loss(vt_pred, images - noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        training_log.append({'epoch': epoch + 1, 'loss': avg_loss})\n",
    "        print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save loss log\n",
    "        with open(os.path.join(RESULTS_DIR, 'training_log.json'), 'w') as f:\n",
    "            json.dump(training_log, f, indent=4)\n",
    "        \n",
    "        # Generate samples\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            visualize_train(epoch + 1)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Training complete. Model saved to: {model_save_path}\")\n",
    "    \n",
    "    # Plot training curve\n",
    "    plot_training_curve(training_log)\n",
    "\n",
    "\n",
    "def plot_training_curve(training_log):\n",
    "    \"\"\"Plot and save training loss curve\"\"\"\n",
    "    epochs_list = [entry['epoch'] for entry in training_log]\n",
    "    losses = [entry['loss'] for entry in training_log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_list, losses, linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title(f'Training Loss - {EXPERIMENT_NAME}', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'training_curve.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Training curve saved to: {os.path.join(RESULTS_DIR, 'training_curve.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_dataset)} training images\")\n",
    "print(f\"Image shape: {channels} x {image_size} x {image_size}\")\n",
    "\n",
    "# Train model\n",
    "train(epochs)\n",
    "\n",
    "# Generate final samples\n",
    "generate_final_samples(num_sample=5)\n",
    "\n",
    "print(f\"\\nExperiment '{EXPERIMENT_NAME}' completed!\")\n",
    "print(f\"All results saved in: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231bfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad701a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
