{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 469/469 [00:24<00:00, 19.13it/s, Loss=925.8238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 469/469 [00:24<00:00, 19.07it/s, Loss=689.1275]\n",
      "Epoch 3: 100%|██████████| 469/469 [00:24<00:00, 19.11it/s, Loss=590.8281]\n",
      "Epoch 4: 100%|██████████| 469/469 [00:24<00:00, 18.97it/s, Loss=521.7590]\n",
      "Epoch 5: 100%|██████████| 469/469 [00:24<00:00, 18.88it/s, Loss=460.5462]\n",
      "Epoch 6: 100%|██████████| 469/469 [00:24<00:00, 19.13it/s, Loss=405.8646]\n",
      "Epoch 7: 100%|██████████| 469/469 [00:24<00:00, 19.04it/s, Loss=361.4963]\n",
      "Epoch 8: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=325.7300]\n",
      "Epoch 9: 100%|██████████| 469/469 [00:24<00:00, 19.05it/s, Loss=300.4889]\n",
      "Epoch 10: 100%|██████████| 469/469 [00:24<00:00, 19.16it/s, Loss=281.4976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 469/469 [00:24<00:00, 19.13it/s, Loss=266.2280]\n",
      "Epoch 12: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=255.5439]\n",
      "Epoch 13: 100%|██████████| 469/469 [00:24<00:00, 19.17it/s, Loss=246.8708]\n",
      "Epoch 14: 100%|██████████| 469/469 [00:24<00:00, 19.08it/s, Loss=240.5798]\n",
      "Epoch 15: 100%|██████████| 469/469 [00:24<00:00, 19.20it/s, Loss=234.5871]\n",
      "Epoch 16: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=230.1819]\n",
      "Epoch 17: 100%|██████████| 469/469 [00:24<00:00, 19.16it/s, Loss=224.9269]\n",
      "Epoch 18: 100%|██████████| 469/469 [00:24<00:00, 19.19it/s, Loss=221.1591]\n",
      "Epoch 19: 100%|██████████| 469/469 [00:24<00:00, 19.11it/s, Loss=217.9509]\n",
      "Epoch 20: 100%|██████████| 469/469 [00:24<00:00, 19.08it/s, Loss=214.3570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 469/469 [00:24<00:00, 19.19it/s, Loss=211.5842]\n",
      "Epoch 22: 100%|██████████| 469/469 [00:24<00:00, 19.11it/s, Loss=208.6239]\n",
      "Epoch 23: 100%|██████████| 469/469 [00:24<00:00, 19.12it/s, Loss=206.8436]\n",
      "Epoch 24: 100%|██████████| 469/469 [00:24<00:00, 19.22it/s, Loss=203.2784]\n",
      "Epoch 25: 100%|██████████| 469/469 [00:24<00:00, 19.04it/s, Loss=201.6637]\n",
      "Epoch 26: 100%|██████████| 469/469 [00:48<00:00,  9.71it/s, Loss=199.1758]\n",
      "Epoch 27: 100%|██████████| 469/469 [00:36<00:00, 12.79it/s, Loss=197.5957]\n",
      "Epoch 28: 100%|██████████| 469/469 [00:50<00:00,  9.20it/s, Loss=194.9864]\n",
      "Epoch 29: 100%|██████████| 469/469 [00:47<00:00,  9.97it/s, Loss=193.4212]\n",
      "Epoch 30: 100%|██████████| 469/469 [00:24<00:00, 18.86it/s, Loss=192.2274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 469/469 [00:24<00:00, 19.08it/s, Loss=189.6132]\n",
      "Epoch 32: 100%|██████████| 469/469 [00:24<00:00, 19.01it/s, Loss=187.9532]\n",
      "Epoch 33: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=186.5910]\n",
      "Epoch 34: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=185.0160]\n",
      "Epoch 35: 100%|██████████| 469/469 [00:24<00:00, 19.11it/s, Loss=183.4117]\n",
      "Epoch 36: 100%|██████████| 469/469 [00:24<00:00, 19.05it/s, Loss=182.1165]\n",
      "Epoch 37: 100%|██████████| 469/469 [00:24<00:00, 19.04it/s, Loss=180.8234]\n",
      "Epoch 38: 100%|██████████| 469/469 [00:24<00:00, 19.12it/s, Loss=179.3385]\n",
      "Epoch 39: 100%|██████████| 469/469 [00:24<00:00, 19.16it/s, Loss=177.9864]\n",
      "Epoch 40: 100%|██████████| 469/469 [00:24<00:00, 19.05it/s, Loss=176.0617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 469/469 [00:24<00:00, 19.07it/s, Loss=175.8308]\n",
      "Epoch 42: 100%|██████████| 469/469 [00:24<00:00, 19.07it/s, Loss=174.2884]\n",
      "Epoch 43: 100%|██████████| 469/469 [00:24<00:00, 19.21it/s, Loss=173.4384]\n",
      "Epoch 44: 100%|██████████| 469/469 [00:24<00:00, 19.09it/s, Loss=171.8537]\n",
      "Epoch 45: 100%|██████████| 469/469 [00:24<00:00, 19.21it/s, Loss=171.2573]\n",
      "Epoch 46: 100%|██████████| 469/469 [00:24<00:00, 19.15it/s, Loss=170.0746]\n",
      "Epoch 47: 100%|██████████| 469/469 [00:24<00:00, 19.17it/s, Loss=168.8070]\n",
      "Epoch 48: 100%|██████████| 469/469 [00:24<00:00, 19.20it/s, Loss=167.9851]\n",
      "Epoch 49: 100%|██████████| 469/469 [00:24<00:00, 19.22it/s, Loss=167.0389]\n",
      "Epoch 50: 100%|██████████| 469/469 [00:24<00:00, 19.22it/s, Loss=166.2858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training visualization samples...\n",
      "Training complete. Model saved to: FMmodel.pth\n",
      "Loading model from FMmodel.pth and generating 5 images...\n",
      "Generating image 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images for each digit: 100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved to: generated_image1.jpg\n",
      "Generating image 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images for each digit: 100%|██████████| 10/10 [00:01<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved to: generated_image2.jpg\n",
      "Generating image 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images for each digit: 100%|██████████| 10/10 [00:02<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved to: generated_image3.jpg\n",
      "Generating image 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images for each digit: 100%|██████████| 10/10 [00:01<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved to: generated_image4.jpg\n",
      "Generating image 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images for each digit: 100%|██████████| 10/10 [00:01<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved to: generated_image5.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================== Configuration Parameters ==================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "epochs = 50\n",
    "num_classes = 10\n",
    "model_save_path = 'FMmodel.pth'\n",
    "\n",
    "# ================== Data Loading ==================\n",
    "\n",
    "# Normalize image to [-1,1]\n",
    "def normalize_img(x):\n",
    "    return 2 * x - 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),              # Convert image to tensor\n",
    "    transforms.Lambda(normalize_img)    # Apply normalization\n",
    "])\n",
    "\n",
    "# ================== Model Architecture ==================\n",
    "class ConditionedDoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution module with condition injection\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)     #TODO: why even or odd.\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)                                      #TODO: what about different norms. Group and Batch and Layer\n",
    "        self.conv2 = nn.Conv2d(out_channels + cond_dim, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = F.silu(self.norm1(self.conv1(x)))           #TODO: other activations #TODO: conv -> act -> norm?\n",
    "        cond = cond.expand(-1, -1, x.size(2), x.size(3))  # Dynamic condition broadcasting\n",
    "        x = torch.cat([x, cond], dim=1)\n",
    "        return F.silu(self.norm2(self.conv2(x)))\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d(2)                  #TODO: other downsampling methods (avg.)\n",
    "        self.conv = ConditionedDoubleConv(in_channels, out_channels, cond_dim)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = self.maxpool(x)\n",
    "        return self.conv(x, cond)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)      #TODO: better alternative (transposed conv)\n",
    "        self.conv = ConditionedDoubleConv(in_channels, out_channels, cond_dim)\n",
    "\n",
    "    def forward(self, x1, x2, cond):\n",
    "        x1 = self.up(x1)\n",
    "        # Size alignment\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x, cond)\n",
    "\n",
    "\n",
    "#TODO: Diffusion/Vision Transformers instead of UNet\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"Dimension-safe conditional UNet\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Unified condition encoding dimensions\n",
    "        self.t_dim = 16\n",
    "        self.label_dim = 16\n",
    "        self.cond_dim = self.t_dim + self.label_dim  # 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(32, self.t_dim)\n",
    "        )\n",
    "        # Label embedding\n",
    "        self.label_embed = nn.Embedding(num_classes, self.label_dim)\n",
    "\n",
    "        # Encoder path\n",
    "        self.inc = ConditionedDoubleConv(1, 64, self.cond_dim)\n",
    "        self.down1 = Down(64, 128, self.cond_dim)\n",
    "        self.down2 = Down(128, 256, self.cond_dim)\n",
    "\n",
    "        # Decoder path\n",
    "        self.up1 = Up(256 + 128, 128, self.cond_dim)  # Input channel correction\n",
    "        self.up2 = Up(128 + 64, 64, self.cond_dim)\n",
    "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        # Condition encoding (unified dimensions)\n",
    "        t_emb = self.time_embed(t.view(-1, 1))  # [B, 16]\n",
    "        lbl_emb = self.label_embed(labels)  # [B, 16]\n",
    "        cond = torch.cat([t_emb, lbl_emb], dim=1)  # [B, 32]\n",
    "        cond = cond.unsqueeze(-1).unsqueeze(-1)  # [B, 32, 1, 1]\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.inc(x, cond)\n",
    "        x2 = self.down1(x1, cond)\n",
    "        x3 = self.down2(x2, cond)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x3, x2, cond)\n",
    "        x = self.up2(x, x1, cond)\n",
    "        return self.outc(x)\n",
    "\n",
    "\n",
    "# ================== Training and Generation ==================\n",
    "# Initialize model and optimizer here for global access (especially for generate_with_label)\n",
    "model = ConditionalUNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_label(label, num_samples=16):\n",
    "    \"\"\"\n",
    "    Generate samples with specified label.\n",
    "    Args:\n",
    "        label (int): Digit label to generate (0-9).\n",
    "        num_samples (int): Number of samples to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: Generated image tensor, shape (num_samples, image_size, image_size), values in [0, 1].\n",
    "    \"\"\"\n",
    "    # Save current model training state and set to eval mode\n",
    "    current_model_state = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # Create initial noise and label tensors\n",
    "    x0 = torch.randn(num_samples, 1, image_size, image_size, device=device)\n",
    "    labels = torch.full((num_samples,), label, device=device, dtype=torch.long)\n",
    "\n",
    "    # ODE: t is time, x is current state\n",
    "    def ode_func(t: torch.Tensor, x: torch.Tensor):\n",
    "        t_expanded = t.expand(x.size(0))  # [1] -> [num_samples]\n",
    "        vt = model(x, t_expanded, labels)  # Predict velocity field\n",
    "        return vt\n",
    "\n",
    "    # Time points: 0 -> 1\n",
    "    t_eval = torch.tensor([0.0, 1.0], device=device)\n",
    "\n",
    "    # Solve ODE (adaptive step size)            #TODO: change ode solvers\n",
    "    generated = odeint(\n",
    "        ode_func,\n",
    "        x0,\n",
    "        t_eval,\n",
    "        rtol=1e-5,\n",
    "        atol=1e-5,\n",
    "        method='dopri5'\n",
    "    )\n",
    "\n",
    "    # Restore training state\n",
    "    model.train(current_model_state)\n",
    "\n",
    "    # Post-processing\n",
    "    images = (generated[-1].clamp(-1, 1) + 1) / 2  # [-1,1] -> [0,1]\n",
    "    return images.cpu().squeeze(1)\n",
    "\n",
    "\n",
    "def visualize_train(epoch):\n",
    "    \"\"\"\n",
    "    Generate visualization using current model:\n",
    "    Create a 10x10 grid with columns 1-10 for each digit (0-9),\n",
    "    generating 10 images per digit with labels in the first row.\n",
    "    \"\"\"\n",
    "    print(\"Generating training visualization samples...\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)  # Reduce subplot spacing\n",
    "\n",
    "    # Generate 10 images for each digit 0-9\n",
    "    for label in range(num_classes):\n",
    "        # Generate 10 samples for current digit\n",
    "        generated_images = generate_with_label(\n",
    "            label=label,\n",
    "            num_samples=10\n",
    "        ).numpy()  # Shape (10, 28, 28)\n",
    "\n",
    "        # Plot in current column, ensuring each column represents one digit\n",
    "        for i in range(10):\n",
    "            # Subplot position calculation: (row_index * total_columns) + column_index + 1\n",
    "            # We want column 0 for digit 0, column 1 for digit 1, etc.\n",
    "            # So: (row i * num_classes (10)) + column label + 1\n",
    "            ax = plt.subplot(10, num_classes, (i * num_classes) + label + 1)\n",
    "            plt.imshow(generated_images[i], cmap='gray', vmin=0, vmax=1)\n",
    "            ax.axis('off')\n",
    "            # Add digit label in first row of each column (when i == 0)\n",
    "            if i == 0:\n",
    "                ax.set_title(str(label), fontsize=16, pad=5)  # set_title is more appropriate\n",
    "    plt.suptitle(\"Generated Samples During Training\", fontsize=20, y=0.97)\n",
    "    plt.savefig(f\"epoch{epoch}.jpg\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def hundred_image(model_path=model_save_path, num_sample=5):\n",
    "    print(f\"Loading model from {model_path} and generating {num_sample} images...\")\n",
    "    # Create new model instance and load weights\n",
    "    global model\n",
    "    original_model = model  # Save original model reference\n",
    "\n",
    "    loaded_model = ConditionalUNet().to(device)\n",
    "    if os.path.exists(model_path):\n",
    "        loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        loaded_model.eval()  # Evaluation mode\n",
    "    else:\n",
    "        print(f\"Error: Model file not found: {model_path}. Cannot generate images.\")\n",
    "        return\n",
    "\n",
    "    model = loaded_model  # Temporarily replace global model with loaded model\n",
    "\n",
    "    for k in range(num_sample):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        print(f\"Generating image {k + 1}...\")\n",
    "        for label in tqdm(range(num_classes), desc=\"Generating images for each digit\"):\n",
    "            # Generate 10 samples for current digit\n",
    "            generated_images = generate_with_label(\n",
    "                label=label,\n",
    "                num_samples=10\n",
    "            ).numpy()  # Shape (10, 28, 28)\n",
    "\n",
    "            # Plot in current column\n",
    "            for i in range(10):\n",
    "                ax = plt.subplot(10, num_classes, (i * num_classes) + label + 1)\n",
    "                plt.imshow(generated_images[i], cmap='gray', vmin=0, vmax=1)\n",
    "                ax.axis('off')\n",
    "                # Add label in first row\n",
    "                if i == 0:\n",
    "                    ax.set_title(str(label), fontsize=16, pad=5)\n",
    "\n",
    "        plt.suptitle(f\"Final Generated Samples (Generation {k + 1})\", fontsize=20, y=0.97)\n",
    "        plt.savefig(f\"generated_image{k + 1}.jpg\")\n",
    "        print(f\"Generated image saved to: generated_image{k + 1}.jpg\")\n",
    "        plt.close()\n",
    "\n",
    "    # Restore global model reference\n",
    "    model = original_model\n",
    "\n",
    "\n",
    "def train(num_epochs=100):\n",
    "    \"\"\"Training loop\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    # Ensure train_loader is available here since it's initialized in the if __name__ == \"__main__\": block\n",
    "    global train_loader\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Use tqdm to show training progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "        model.train()  # Training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in progress_bar:  # Add progress bar\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Dynamic noise generation\n",
    "            noise = torch.randn_like(images)\n",
    "            t = torch.rand(images.size(0), device=device)\n",
    "            # Flow Matching target velocity is the velocity field from noise x0 to real data x1\n",
    "            # x_t = (1-t) * x0 + t * x1\n",
    "            xt = (1 - t.view(-1, 1, 1, 1)) * noise + t.view(-1, 1, 1, 1) * images\n",
    "\n",
    "            # Forward pass, model predicts velocity field v_t\n",
    "            vt_pred = model(xt, t, labels)\n",
    "            # True velocity field v_t = x1 - x0\n",
    "            loss = F.mse_loss(vt_pred, images - noise)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping to prevent explosion   #TODO: avoid clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar with loss display\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{total_loss:.4f}\"})\n",
    "\n",
    "        # Generate samples every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_train(epoch + 1)\n",
    "        # Generate sample for first epoch\n",
    "        if epoch == 0:\n",
    "            visualize_train(epoch + 1)\n",
    "\n",
    "    # Save model after training\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Training complete. Model saved to: {model_save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=2)\n",
    "    # Comment out when using existing model\n",
    "    train(epochs)\n",
    "    hundred_image(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
