{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e667281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298edc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'darcy/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = 'darcy/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 250\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "r = 5\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MatReader(TRAIN_PATH)\n",
    "x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,::r,::r][:,:s,:s]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_test = x_test.reshape(ntest,s,s,1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5a027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a937fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_list = []\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        if not self.training:\n",
    "            x_list.append(x[..., :-self.padding, :-self.padding].detach().clone())\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        if not self.training:\n",
    "            x_list.append(x[..., :-self.padding, :-self.padding].detach().clone())\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        if not self.training:\n",
    "            x_list.append(x[..., :-self.padding, :-self.padding].detach().clone())\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        if not self.training:\n",
    "            for i in range(len(x_list)):\n",
    "                x_list[i] = x_list[i].permute(0, 2, 3, 1)\n",
    "                x_list[i] = self.fc1(x_list[i])\n",
    "                x_list[i] = F.gelu(x_list[i])\n",
    "                x_list[i] = self.fc2(x_list[i])\n",
    "\n",
    "        return x, x_list\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24d3ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pinakinchoudhary/dev/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188353\n",
      "Epoch 0: Time 3.50s, Train L2 Loss: 0.1623, Intermediate losses: [10.300284423828124, 7.337637939453125, 5.276469116210937], Test L2 Loss: 0.1143\n",
      "Epoch 1: Time 3.30s, Train L2 Loss: 0.1023, Intermediate losses: [11.0690283203125, 7.506444702148437, 5.172318115234375], Test L2 Loss: 0.0939\n",
      "Epoch 2: Time 3.32s, Train L2 Loss: 0.0734, Intermediate losses: [13.160272216796875, 8.71386474609375, 4.341381225585938], Test L2 Loss: 0.0612\n",
      "Epoch 3: Time 3.33s, Train L2 Loss: 0.0534, Intermediate losses: [14.053746337890624, 9.61626220703125, 4.223255615234375], Test L2 Loss: 0.0489\n",
      "Epoch 4: Time 3.33s, Train L2 Loss: 0.0428, Intermediate losses: [14.73343505859375, 10.5700244140625, 4.071277160644531], Test L2 Loss: 0.0398\n",
      "Epoch 5: Time 3.33s, Train L2 Loss: 0.0361, Intermediate losses: [15.15748291015625, 10.9986181640625, 3.8894854736328126], Test L2 Loss: 0.0373\n",
      "Epoch 6: Time 3.38s, Train L2 Loss: 0.0316, Intermediate losses: [15.77085693359375, 11.419732666015625, 4.115193786621094], Test L2 Loss: 0.0326\n",
      "Epoch 7: Time 3.39s, Train L2 Loss: 0.0272, Intermediate losses: [16.16239501953125, 11.65583984375, 4.145647583007812], Test L2 Loss: 0.0295\n",
      "Epoch 8: Time 3.35s, Train L2 Loss: 0.0248, Intermediate losses: [16.615799560546876, 11.76314208984375, 4.222555541992188], Test L2 Loss: 0.0262\n",
      "Epoch 9: Time 3.34s, Train L2 Loss: 0.0229, Intermediate losses: [17.0825439453125, 12.03797119140625, 4.183421630859375], Test L2 Loss: 0.0305\n",
      "Epoch 10: Time 3.35s, Train L2 Loss: 0.0216, Intermediate losses: [17.23627197265625, 11.899521484375, 4.254830322265625], Test L2 Loss: 0.0241\n",
      "Epoch 11: Time 3.36s, Train L2 Loss: 0.0211, Intermediate losses: [17.14515869140625, 11.582978515625, 4.246085815429687], Test L2 Loss: 0.0260\n",
      "Epoch 12: Time 3.38s, Train L2 Loss: 0.0193, Intermediate losses: [17.475645751953124, 11.588883056640626, 4.35609619140625], Test L2 Loss: 0.0237\n",
      "Epoch 13: Time 3.37s, Train L2 Loss: 0.0179, Intermediate losses: [17.8831396484375, 11.780047607421874, 4.444779052734375], Test L2 Loss: 0.0213\n",
      "Epoch 14: Time 3.38s, Train L2 Loss: 0.0180, Intermediate losses: [17.860943603515626, 11.528460693359374, 4.419008483886719], Test L2 Loss: 0.0206\n",
      "Epoch 15: Time 3.38s, Train L2 Loss: 0.0166, Intermediate losses: [17.939207763671874, 11.47925048828125, 4.4662255859375], Test L2 Loss: 0.0228\n",
      "Epoch 16: Time 3.40s, Train L2 Loss: 0.0162, Intermediate losses: [18.188037109375, 11.55121337890625, 4.455735168457031], Test L2 Loss: 0.0191\n",
      "Epoch 17: Time 3.40s, Train L2 Loss: 0.0168, Intermediate losses: [18.1211572265625, 11.355673828125, 4.414691162109375], Test L2 Loss: 0.0193\n",
      "Epoch 18: Time 3.40s, Train L2 Loss: 0.0148, Intermediate losses: [18.24871826171875, 11.40484130859375, 4.481900329589844], Test L2 Loss: 0.0208\n",
      "Epoch 19: Time 3.40s, Train L2 Loss: 0.0153, Intermediate losses: [18.41236572265625, 11.28851318359375, 4.483164367675781], Test L2 Loss: 0.0175\n",
      "Epoch 20: Time 3.42s, Train L2 Loss: 0.0141, Intermediate losses: [18.593411865234376, 11.3352880859375, 4.467986450195313], Test L2 Loss: 0.0181\n",
      "Epoch 21: Time 3.45s, Train L2 Loss: 0.0134, Intermediate losses: [18.54711669921875, 11.2250537109375, 4.465869445800781], Test L2 Loss: 0.0180\n",
      "Epoch 22: Time 3.43s, Train L2 Loss: 0.0136, Intermediate losses: [18.4214306640625, 11.017039794921875, 4.442874450683593], Test L2 Loss: 0.0185\n",
      "Epoch 23: Time 3.53s, Train L2 Loss: 0.0135, Intermediate losses: [18.89478759765625, 11.319742431640625, 4.502017517089843], Test L2 Loss: 0.0182\n",
      "Epoch 24: Time 3.52s, Train L2 Loss: 0.0126, Intermediate losses: [18.81590576171875, 11.1442724609375, 4.462107849121094], Test L2 Loss: 0.0163\n",
      "Epoch 25: Time 3.54s, Train L2 Loss: 0.0119, Intermediate losses: [19.13458740234375, 11.291767578125, 4.525603637695313], Test L2 Loss: 0.0156\n",
      "Epoch 26: Time 3.55s, Train L2 Loss: 0.0125, Intermediate losses: [19.021629638671875, 11.1939892578125, 4.495499877929688], Test L2 Loss: 0.0160\n",
      "Epoch 27: Time 3.56s, Train L2 Loss: 0.0116, Intermediate losses: [19.26153076171875, 11.218115234375, 4.563661499023437], Test L2 Loss: 0.0159\n",
      "Epoch 28: Time 3.55s, Train L2 Loss: 0.0126, Intermediate losses: [19.207579345703124, 11.0930126953125, 4.518634948730469], Test L2 Loss: 0.0154\n",
      "Epoch 29: Time 3.58s, Train L2 Loss: 0.0123, Intermediate losses: [19.15606201171875, 10.983046875, 4.459561157226562], Test L2 Loss: 0.0168\n",
      "Epoch 30: Time 3.55s, Train L2 Loss: 0.0122, Intermediate losses: [19.327501220703127, 10.988134765625, 4.550063781738281], Test L2 Loss: 0.0150\n",
      "Epoch 31: Time 3.57s, Train L2 Loss: 0.0119, Intermediate losses: [19.19958740234375, 10.681903076171874, 4.465097351074219], Test L2 Loss: 0.0185\n",
      "Epoch 32: Time 3.57s, Train L2 Loss: 0.0116, Intermediate losses: [19.41783203125, 10.81798095703125, 4.509279479980469], Test L2 Loss: 0.0154\n",
      "Epoch 33: Time 3.58s, Train L2 Loss: 0.0113, Intermediate losses: [19.46718994140625, 10.806627197265625, 4.537315368652344], Test L2 Loss: 0.0147\n",
      "Epoch 34: Time 3.56s, Train L2 Loss: 0.0113, Intermediate losses: [19.591142578125, 10.77492919921875, 4.529583740234375], Test L2 Loss: 0.0147\n",
      "Epoch 35: Time 3.58s, Train L2 Loss: 0.0104, Intermediate losses: [19.7389404296875, 10.878656005859375, 4.55608154296875], Test L2 Loss: 0.0140\n",
      "Epoch 36: Time 3.57s, Train L2 Loss: 0.0098, Intermediate losses: [19.894901123046875, 10.98000244140625, 4.592290649414062], Test L2 Loss: 0.0136\n",
      "Epoch 37: Time 3.57s, Train L2 Loss: 0.0109, Intermediate losses: [19.939171142578125, 10.919208984375, 4.59240234375], Test L2 Loss: 0.0144\n",
      "Epoch 38: Time 3.60s, Train L2 Loss: 0.0109, Intermediate losses: [19.869361572265625, 10.800841064453126, 4.571278076171875], Test L2 Loss: 0.0145\n",
      "Epoch 39: Time 3.58s, Train L2 Loss: 0.0100, Intermediate losses: [20.027314453125, 10.8435205078125, 4.567299499511718], Test L2 Loss: 0.0144\n",
      "Epoch 40: Time 3.56s, Train L2 Loss: 0.0101, Intermediate losses: [20.316649169921874, 11.02294677734375, 4.648258056640625], Test L2 Loss: 0.0145\n",
      "Epoch 41: Time 3.58s, Train L2 Loss: 0.0100, Intermediate losses: [20.154525146484374, 10.86886474609375, 4.582920227050781], Test L2 Loss: 0.0140\n",
      "Epoch 42: Time 3.59s, Train L2 Loss: 0.0095, Intermediate losses: [20.351702880859374, 10.94080078125, 4.61790771484375], Test L2 Loss: 0.0130\n",
      "Epoch 43: Time 3.54s, Train L2 Loss: 0.0099, Intermediate losses: [20.35924560546875, 10.86739501953125, 4.563746337890625], Test L2 Loss: 0.0148\n",
      "Epoch 44: Time 3.59s, Train L2 Loss: 0.0100, Intermediate losses: [20.44882568359375, 10.81542724609375, 4.62735107421875], Test L2 Loss: 0.0131\n",
      "Epoch 45: Time 3.60s, Train L2 Loss: 0.0093, Intermediate losses: [20.58382568359375, 10.8521875, 4.6426708984375], Test L2 Loss: 0.0135\n",
      "Epoch 46: Time 3.59s, Train L2 Loss: 0.0097, Intermediate losses: [20.58212890625, 10.808763427734375, 4.63800048828125], Test L2 Loss: 0.0140\n",
      "Epoch 47: Time 3.57s, Train L2 Loss: 0.0095, Intermediate losses: [20.7173681640625, 10.80732421875, 4.639979858398437], Test L2 Loss: 0.0138\n",
      "Epoch 48: Time 3.59s, Train L2 Loss: 0.0116, Intermediate losses: [20.73140625, 10.682613525390625, 4.674237670898438], Test L2 Loss: 0.0132\n",
      "Epoch 49: Time 3.64s, Train L2 Loss: 0.0102, Intermediate losses: [20.60976806640625, 10.562379150390624, 4.638436279296875], Test L2 Loss: 0.0130\n",
      "Epoch 50: Time 3.61s, Train L2 Loss: 0.0094, Intermediate losses: [20.799560546875, 10.752266845703126, 4.702939453125], Test L2 Loss: 0.0159\n",
      "Epoch 51: Time 3.56s, Train L2 Loss: 0.0103, Intermediate losses: [20.65064453125, 10.5353173828125, 4.664736938476563], Test L2 Loss: 0.0136\n",
      "Epoch 52: Time 3.55s, Train L2 Loss: 0.0098, Intermediate losses: [20.65674560546875, 10.48822265625, 4.665559692382812], Test L2 Loss: 0.0128\n",
      "Epoch 53: Time 3.66s, Train L2 Loss: 0.0086, Intermediate losses: [20.8465185546875, 10.58109375, 4.704172973632812], Test L2 Loss: 0.0128\n",
      "Epoch 54: Time 3.61s, Train L2 Loss: 0.0085, Intermediate losses: [20.8746728515625, 10.636884765625, 4.699997863769531], Test L2 Loss: 0.0124\n",
      "Epoch 55: Time 3.58s, Train L2 Loss: 0.0084, Intermediate losses: [21.025107421875, 10.706951904296876, 4.719583129882812], Test L2 Loss: 0.0125\n",
      "Epoch 56: Time 3.55s, Train L2 Loss: 0.0085, Intermediate losses: [20.878544921875, 10.590791015625, 4.665460205078125], Test L2 Loss: 0.0137\n",
      "Epoch 57: Time 3.54s, Train L2 Loss: 0.0092, Intermediate losses: [21.17680419921875, 10.675506591796875, 4.748023681640625], Test L2 Loss: 0.0120\n",
      "Epoch 58: Time 3.63s, Train L2 Loss: 0.0090, Intermediate losses: [21.23928955078125, 10.702454833984374, 4.722341918945313], Test L2 Loss: 0.0135\n",
      "Epoch 59: Time 3.58s, Train L2 Loss: 0.0101, Intermediate losses: [21.3278759765625, 10.664549560546876, 4.776357421875], Test L2 Loss: 0.0124\n",
      "Epoch 60: Time 3.63s, Train L2 Loss: 0.0095, Intermediate losses: [21.29017578125, 10.549990234375, 4.727377014160156], Test L2 Loss: 0.0128\n",
      "Epoch 61: Time 3.66s, Train L2 Loss: 0.0087, Intermediate losses: [21.27447509765625, 10.537310791015624, 4.728735656738281], Test L2 Loss: 0.0131\n",
      "Epoch 62: Time 3.60s, Train L2 Loss: 0.0088, Intermediate losses: [21.46216796875, 10.568292236328125, 4.78074462890625], Test L2 Loss: 0.0147\n",
      "Epoch 63: Time 3.60s, Train L2 Loss: 0.0088, Intermediate losses: [21.5003076171875, 10.644769287109375, 4.795555419921875], Test L2 Loss: 0.0132\n",
      "Epoch 64: Time 3.65s, Train L2 Loss: 0.0084, Intermediate losses: [21.580458984375, 10.67063232421875, 4.804222106933594], Test L2 Loss: 0.0129\n",
      "Epoch 65: Time 3.62s, Train L2 Loss: 0.0097, Intermediate losses: [21.45935791015625, 10.50669189453125, 4.784063720703125], Test L2 Loss: 0.0129\n",
      "Epoch 66: Time 3.60s, Train L2 Loss: 0.0080, Intermediate losses: [21.57287353515625, 10.526151123046875, 4.785278930664062], Test L2 Loss: 0.0120\n",
      "Epoch 67: Time 3.56s, Train L2 Loss: 0.0089, Intermediate losses: [21.6085693359375, 10.537193603515625, 4.822357788085937], Test L2 Loss: 0.0122\n",
      "Epoch 68: Time 3.55s, Train L2 Loss: 0.0082, Intermediate losses: [21.7238720703125, 10.571473388671874, 4.834378967285156], Test L2 Loss: 0.0130\n",
      "Epoch 69: Time 3.59s, Train L2 Loss: 0.0083, Intermediate losses: [21.870498046875, 10.72368408203125, 4.865186157226563], Test L2 Loss: 0.0120\n",
      "Epoch 70: Time 3.61s, Train L2 Loss: 0.0088, Intermediate losses: [21.913154296875, 10.66049072265625, 4.864453735351563], Test L2 Loss: 0.0124\n",
      "Epoch 71: Time 3.58s, Train L2 Loss: 0.0085, Intermediate losses: [21.86158203125, 10.595888671875, 4.850765075683594], Test L2 Loss: 0.0119\n",
      "Epoch 72: Time 3.55s, Train L2 Loss: 0.0090, Intermediate losses: [21.9871435546875, 10.6117626953125, 4.889254150390625], Test L2 Loss: 0.0117\n",
      "Epoch 73: Time 3.54s, Train L2 Loss: 0.0084, Intermediate losses: [22.08820556640625, 10.62246337890625, 4.903898620605469], Test L2 Loss: 0.0120\n",
      "Epoch 74: Time 3.62s, Train L2 Loss: 0.0089, Intermediate losses: [22.00792724609375, 10.45486572265625, 4.872481689453125], Test L2 Loss: 0.0125\n",
      "Epoch 75: Time 3.64s, Train L2 Loss: 0.0089, Intermediate losses: [21.9967724609375, 10.4744140625, 4.887669677734375], Test L2 Loss: 0.0123\n",
      "Epoch 76: Time 3.65s, Train L2 Loss: 0.0077, Intermediate losses: [22.22311767578125, 10.596396484375, 4.9378125], Test L2 Loss: 0.0112\n",
      "Epoch 77: Time 3.57s, Train L2 Loss: 0.0075, Intermediate losses: [22.2537109375, 10.625146484375, 4.94175537109375], Test L2 Loss: 0.0116\n",
      "Epoch 78: Time 3.62s, Train L2 Loss: 0.0077, Intermediate losses: [22.2222607421875, 10.55140380859375, 4.913206176757813], Test L2 Loss: 0.0114\n",
      "Epoch 79: Time 3.58s, Train L2 Loss: 0.0081, Intermediate losses: [22.4779541015625, 10.627763671875, 4.959164123535157], Test L2 Loss: 0.0125\n",
      "Epoch 80: Time 3.59s, Train L2 Loss: 0.0083, Intermediate losses: [22.5165380859375, 10.628018798828125, 4.958234252929688], Test L2 Loss: 0.0111\n",
      "Epoch 81: Time 3.61s, Train L2 Loss: 0.0076, Intermediate losses: [22.5617724609375, 10.65132080078125, 4.964122009277344], Test L2 Loss: 0.0111\n",
      "Epoch 82: Time 3.62s, Train L2 Loss: 0.0071, Intermediate losses: [22.75692626953125, 10.6935595703125, 4.976543884277344], Test L2 Loss: 0.0114\n",
      "Epoch 83: Time 3.59s, Train L2 Loss: 0.0074, Intermediate losses: [22.8058056640625, 10.799017333984375, 5.01290283203125], Test L2 Loss: 0.0109\n",
      "Epoch 84: Time 3.65s, Train L2 Loss: 0.0074, Intermediate losses: [22.8052490234375, 10.80907958984375, 5.023592529296875], Test L2 Loss: 0.0115\n",
      "Epoch 85: Time 3.59s, Train L2 Loss: 0.0078, Intermediate losses: [22.90164794921875, 10.79481689453125, 5.046561279296875], Test L2 Loss: 0.0122\n",
      "Epoch 86: Time 3.57s, Train L2 Loss: 0.0076, Intermediate losses: [23.0270458984375, 10.773609619140625, 5.054342651367188], Test L2 Loss: 0.0124\n",
      "Epoch 87: Time 3.55s, Train L2 Loss: 0.0083, Intermediate losses: [23.0923828125, 10.74484619140625, 5.060878601074219], Test L2 Loss: 0.0116\n",
      "Epoch 88: Time 3.55s, Train L2 Loss: 0.0082, Intermediate losses: [23.01181884765625, 10.723421630859375, 5.054266357421875], Test L2 Loss: 0.0126\n",
      "Epoch 89: Time 3.62s, Train L2 Loss: 0.0078, Intermediate losses: [23.0093359375, 10.67578857421875, 5.033136596679688], Test L2 Loss: 0.0113\n",
      "Epoch 90: Time 3.62s, Train L2 Loss: 0.0075, Intermediate losses: [23.2260546875, 10.70138427734375, 5.048090209960938], Test L2 Loss: 0.0109\n",
      "Epoch 91: Time 3.60s, Train L2 Loss: 0.0085, Intermediate losses: [23.26982421875, 10.613370361328125, 5.051502685546875], Test L2 Loss: 0.0119\n",
      "Epoch 92: Time 3.63s, Train L2 Loss: 0.0087, Intermediate losses: [23.07998046875, 10.55093017578125, 5.046171264648438], Test L2 Loss: 0.0115\n",
      "Epoch 93: Time 3.63s, Train L2 Loss: 0.0079, Intermediate losses: [23.1134033203125, 10.57917724609375, 5.093485107421875], Test L2 Loss: 0.0119\n",
      "Epoch 94: Time 3.60s, Train L2 Loss: 0.0070, Intermediate losses: [23.371796875, 10.70921875, 5.117967224121093], Test L2 Loss: 0.0111\n",
      "Epoch 95: Time 3.61s, Train L2 Loss: 0.0085, Intermediate losses: [23.436689453125, 10.57950439453125, 5.14345947265625], Test L2 Loss: 0.0117\n",
      "Epoch 96: Time 3.63s, Train L2 Loss: 0.0077, Intermediate losses: [23.265341796875, 10.5118017578125, 5.12006103515625], Test L2 Loss: 0.0113\n",
      "Epoch 97: Time 3.59s, Train L2 Loss: 0.0080, Intermediate losses: [23.40435302734375, 10.562003173828124, 5.088068237304688], Test L2 Loss: 0.0115\n",
      "Epoch 98: Time 3.61s, Train L2 Loss: 0.0083, Intermediate losses: [23.39224365234375, 10.508824462890624, 5.115256652832032], Test L2 Loss: 0.0116\n",
      "Epoch 99: Time 3.59s, Train L2 Loss: 0.0084, Intermediate losses: [23.47751708984375, 10.51226318359375, 5.149526977539063], Test L2 Loss: 0.0116\n",
      "Epoch 100: Time 3.63s, Train L2 Loss: 0.0068, Intermediate losses: [23.5106884765625, 10.54687255859375, 5.1584228515625], Test L2 Loss: 0.0101\n",
      "Epoch 101: Time 3.60s, Train L2 Loss: 0.0060, Intermediate losses: [23.58025146484375, 10.633516845703125, 5.174393310546875], Test L2 Loss: 0.0104\n",
      "Epoch 102: Time 3.62s, Train L2 Loss: 0.0058, Intermediate losses: [23.6691943359375, 10.71222412109375, 5.2063916015625], Test L2 Loss: 0.0101\n",
      "Epoch 103: Time 3.57s, Train L2 Loss: 0.0057, Intermediate losses: [23.7733203125, 10.8299560546875, 5.23402587890625], Test L2 Loss: 0.0101\n",
      "Epoch 104: Time 3.54s, Train L2 Loss: 0.0056, Intermediate losses: [23.87833740234375, 10.890302734375, 5.219150390625], Test L2 Loss: 0.0103\n",
      "Epoch 105: Time 3.55s, Train L2 Loss: 0.0056, Intermediate losses: [23.92783935546875, 10.96637451171875, 5.252396240234375], Test L2 Loss: 0.0101\n",
      "Epoch 106: Time 3.54s, Train L2 Loss: 0.0057, Intermediate losses: [24.0394775390625, 11.03025634765625, 5.267935791015625], Test L2 Loss: 0.0100\n",
      "Epoch 107: Time 3.58s, Train L2 Loss: 0.0057, Intermediate losses: [24.18021728515625, 11.12254150390625, 5.28632568359375], Test L2 Loss: 0.0103\n",
      "Epoch 108: Time 3.72s, Train L2 Loss: 0.0059, Intermediate losses: [24.27393798828125, 11.141204833984375, 5.290480346679687], Test L2 Loss: 0.0101\n",
      "Epoch 109: Time 3.61s, Train L2 Loss: 0.0058, Intermediate losses: [24.339296875, 11.18809814453125, 5.296796875], Test L2 Loss: 0.0100\n",
      "Epoch 110: Time 3.58s, Train L2 Loss: 0.0057, Intermediate losses: [24.33720703125, 11.2346630859375, 5.312322998046875], Test L2 Loss: 0.0101\n",
      "Epoch 111: Time 3.59s, Train L2 Loss: 0.0057, Intermediate losses: [24.50183349609375, 11.301314697265624, 5.3292724609375], Test L2 Loss: 0.0098\n",
      "Epoch 112: Time 3.63s, Train L2 Loss: 0.0056, Intermediate losses: [24.60373046875, 11.37757080078125, 5.359017944335937], Test L2 Loss: 0.0100\n",
      "Epoch 113: Time 3.56s, Train L2 Loss: 0.0055, Intermediate losses: [24.6540185546875, 11.42496826171875, 5.375780029296875], Test L2 Loss: 0.0100\n",
      "Epoch 114: Time 3.54s, Train L2 Loss: 0.0055, Intermediate losses: [24.82987060546875, 11.49375732421875, 5.386805419921875], Test L2 Loss: 0.0099\n",
      "Epoch 115: Time 3.55s, Train L2 Loss: 0.0054, Intermediate losses: [24.922568359375, 11.529053955078124, 5.378787231445313], Test L2 Loss: 0.0098\n",
      "Epoch 116: Time 3.68s, Train L2 Loss: 0.0054, Intermediate losses: [24.9627490234375, 11.58899169921875, 5.402575073242187], Test L2 Loss: 0.0101\n",
      "Epoch 117: Time 3.70s, Train L2 Loss: 0.0056, Intermediate losses: [25.1358251953125, 11.641094970703126, 5.4114404296875], Test L2 Loss: 0.0101\n",
      "Epoch 118: Time 3.60s, Train L2 Loss: 0.0055, Intermediate losses: [25.156171875, 11.671669921875, 5.426697998046875], Test L2 Loss: 0.0100\n",
      "Epoch 119: Time 3.54s, Train L2 Loss: 0.0059, Intermediate losses: [25.33772705078125, 11.7513232421875, 5.451812744140625], Test L2 Loss: 0.0099\n",
      "Epoch 120: Time 3.56s, Train L2 Loss: 0.0061, Intermediate losses: [25.30921875, 11.72170654296875, 5.435158081054688], Test L2 Loss: 0.0098\n",
      "Epoch 121: Time 3.58s, Train L2 Loss: 0.0060, Intermediate losses: [25.51821044921875, 11.763509521484375, 5.4508056640625], Test L2 Loss: 0.0099\n",
      "Epoch 122: Time 3.58s, Train L2 Loss: 0.0056, Intermediate losses: [25.5265478515625, 11.807222900390625, 5.46354736328125], Test L2 Loss: 0.0098\n",
      "Epoch 123: Time 3.60s, Train L2 Loss: 0.0054, Intermediate losses: [25.59981689453125, 11.86655517578125, 5.478961181640625], Test L2 Loss: 0.0100\n",
      "Epoch 124: Time 3.65s, Train L2 Loss: 0.0055, Intermediate losses: [25.63686767578125, 11.85527587890625, 5.479844970703125], Test L2 Loss: 0.0100\n",
      "Epoch 125: Time 3.59s, Train L2 Loss: 0.0056, Intermediate losses: [25.8297216796875, 11.93126220703125, 5.498380126953125], Test L2 Loss: 0.0097\n",
      "Epoch 126: Time 3.57s, Train L2 Loss: 0.0056, Intermediate losses: [25.871123046875, 11.9382958984375, 5.507022705078125], Test L2 Loss: 0.0099\n",
      "Epoch 127: Time 3.58s, Train L2 Loss: 0.0060, Intermediate losses: [26.0405615234375, 11.99673095703125, 5.526630859375], Test L2 Loss: 0.0099\n",
      "Epoch 128: Time 3.57s, Train L2 Loss: 0.0057, Intermediate losses: [26.1237451171875, 12.04725830078125, 5.547899780273437], Test L2 Loss: 0.0100\n",
      "Epoch 129: Time 3.60s, Train L2 Loss: 0.0057, Intermediate losses: [26.0928662109375, 12.04627685546875, 5.5395654296875], Test L2 Loss: 0.0097\n",
      "Epoch 130: Time 3.63s, Train L2 Loss: 0.0056, Intermediate losses: [26.2165966796875, 12.07898681640625, 5.552215576171875], Test L2 Loss: 0.0095\n",
      "Epoch 131: Time 3.64s, Train L2 Loss: 0.0058, Intermediate losses: [26.25124755859375, 12.14687744140625, 5.581400146484375], Test L2 Loss: 0.0102\n",
      "Epoch 132: Time 3.60s, Train L2 Loss: 0.0059, Intermediate losses: [26.299638671875, 12.1352294921875, 5.572666015625], Test L2 Loss: 0.0098\n",
      "Epoch 133: Time 3.56s, Train L2 Loss: 0.0061, Intermediate losses: [26.4393359375, 12.1152880859375, 5.579935302734375], Test L2 Loss: 0.0098\n",
      "Epoch 134: Time 3.54s, Train L2 Loss: 0.0059, Intermediate losses: [26.53888916015625, 12.12301025390625, 5.59606201171875], Test L2 Loss: 0.0102\n",
      "Epoch 135: Time 3.61s, Train L2 Loss: 0.0056, Intermediate losses: [26.5323583984375, 12.153929443359376, 5.5958349609375], Test L2 Loss: 0.0095\n",
      "Epoch 136: Time 3.61s, Train L2 Loss: 0.0056, Intermediate losses: [26.6450390625, 12.2248828125, 5.611053466796875], Test L2 Loss: 0.0098\n",
      "Epoch 137: Time 3.58s, Train L2 Loss: 0.0055, Intermediate losses: [26.74290283203125, 12.263792724609376, 5.624471435546875], Test L2 Loss: 0.0097\n",
      "Epoch 138: Time 3.58s, Train L2 Loss: 0.0060, Intermediate losses: [26.7219580078125, 12.287412109375, 5.608827514648437], Test L2 Loss: 0.0103\n",
      "Epoch 139: Time 3.60s, Train L2 Loss: 0.0056, Intermediate losses: [26.8303515625, 12.35460205078125, 5.656060791015625], Test L2 Loss: 0.0100\n",
      "Epoch 140: Time 3.56s, Train L2 Loss: 0.0061, Intermediate losses: [26.94145751953125, 12.3082958984375, 5.656360473632812], Test L2 Loss: 0.0096\n",
      "Epoch 141: Time 3.65s, Train L2 Loss: 0.0053, Intermediate losses: [26.97658203125, 12.33656982421875, 5.654349975585937], Test L2 Loss: 0.0095\n",
      "Epoch 142: Time 3.65s, Train L2 Loss: 0.0053, Intermediate losses: [27.1184423828125, 12.44077392578125, 5.680198364257812], Test L2 Loss: 0.0104\n",
      "Epoch 143: Time 3.58s, Train L2 Loss: 0.0055, Intermediate losses: [27.18278076171875, 12.408740234375, 5.670368041992187], Test L2 Loss: 0.0096\n",
      "Epoch 144: Time 3.58s, Train L2 Loss: 0.0055, Intermediate losses: [27.17813232421875, 12.47451171875, 5.69818359375], Test L2 Loss: 0.0095\n",
      "Epoch 145: Time 3.55s, Train L2 Loss: 0.0053, Intermediate losses: [27.30662109375, 12.50260498046875, 5.704110107421875], Test L2 Loss: 0.0094\n",
      "Epoch 146: Time 3.62s, Train L2 Loss: 0.0051, Intermediate losses: [27.4488427734375, 12.57480224609375, 5.719732666015625], Test L2 Loss: 0.0096\n",
      "Epoch 147: Time 3.57s, Train L2 Loss: 0.0054, Intermediate losses: [27.397314453125, 12.5367431640625, 5.707688598632813], Test L2 Loss: 0.0096\n",
      "Epoch 148: Time 3.71s, Train L2 Loss: 0.0055, Intermediate losses: [27.61734130859375, 12.65514404296875, 5.748580322265625], Test L2 Loss: 0.0097\n",
      "Epoch 149: Time 3.59s, Train L2 Loss: 0.0058, Intermediate losses: [27.614228515625, 12.6108251953125, 5.734419555664062], Test L2 Loss: 0.0097\n",
      "Epoch 150: Time 3.58s, Train L2 Loss: 0.0059, Intermediate losses: [27.66731201171875, 12.63435791015625, 5.749586791992187], Test L2 Loss: 0.0097\n",
      "Epoch 151: Time 3.60s, Train L2 Loss: 0.0058, Intermediate losses: [27.6585009765625, 12.57173583984375, 5.74677490234375], Test L2 Loss: 0.0100\n",
      "Epoch 152: Time 3.67s, Train L2 Loss: 0.0056, Intermediate losses: [27.7444921875, 12.66438720703125, 5.754194946289062], Test L2 Loss: 0.0094\n",
      "Epoch 153: Time 3.53s, Train L2 Loss: 0.0056, Intermediate losses: [27.755009765625, 12.6514208984375, 5.763943481445312], Test L2 Loss: 0.0094\n",
      "Epoch 154: Time 3.57s, Train L2 Loss: 0.0055, Intermediate losses: [27.92857421875, 12.748397216796874, 5.790260009765625], Test L2 Loss: 0.0099\n",
      "Epoch 155: Time 3.63s, Train L2 Loss: 0.0053, Intermediate losses: [27.8902001953125, 12.76245849609375, 5.808677978515625], Test L2 Loss: 0.0098\n",
      "Epoch 156: Time 3.60s, Train L2 Loss: 0.0053, Intermediate losses: [27.9731884765625, 12.756202392578125, 5.805338745117187], Test L2 Loss: 0.0094\n",
      "Epoch 157: Time 3.57s, Train L2 Loss: 0.0053, Intermediate losses: [28.1133544921875, 12.81911376953125, 5.8188104248046875], Test L2 Loss: 0.0093\n",
      "Epoch 158: Time 3.59s, Train L2 Loss: 0.0051, Intermediate losses: [28.17330078125, 12.90119384765625, 5.837706298828125], Test L2 Loss: 0.0096\n",
      "Epoch 159: Time 3.61s, Train L2 Loss: 0.0053, Intermediate losses: [28.27450927734375, 12.914739990234375, 5.8309765625], Test L2 Loss: 0.0097\n",
      "Epoch 160: Time 3.58s, Train L2 Loss: 0.0053, Intermediate losses: [28.29499267578125, 12.95501220703125, 5.87036865234375], Test L2 Loss: 0.0098\n",
      "Epoch 161: Time 3.56s, Train L2 Loss: 0.0055, Intermediate losses: [28.3208642578125, 12.92140625, 5.869761962890625], Test L2 Loss: 0.0095\n",
      "Epoch 162: Time 3.60s, Train L2 Loss: 0.0052, Intermediate losses: [28.4422216796875, 12.96825439453125, 5.865684814453125], Test L2 Loss: 0.0095\n",
      "Epoch 163: Time 3.71s, Train L2 Loss: 0.0058, Intermediate losses: [28.54232177734375, 12.96913330078125, 5.8777294921875], Test L2 Loss: 0.0103\n",
      "Epoch 164: Time 3.61s, Train L2 Loss: 0.0059, Intermediate losses: [28.59506103515625, 13.0046044921875, 5.89146240234375], Test L2 Loss: 0.0096\n",
      "Epoch 165: Time 3.59s, Train L2 Loss: 0.0058, Intermediate losses: [28.73314453125, 12.96986328125, 5.894938354492187], Test L2 Loss: 0.0094\n",
      "Epoch 166: Time 3.57s, Train L2 Loss: 0.0052, Intermediate losses: [28.7002783203125, 13.032109375, 5.925015869140625], Test L2 Loss: 0.0098\n",
      "Epoch 167: Time 3.63s, Train L2 Loss: 0.0054, Intermediate losses: [28.72810546875, 13.03837158203125, 5.927767333984375], Test L2 Loss: 0.0095\n",
      "Epoch 168: Time 3.61s, Train L2 Loss: 0.0052, Intermediate losses: [28.853642578125, 13.054405517578125, 5.9351171875], Test L2 Loss: 0.0092\n",
      "Epoch 169: Time 3.57s, Train L2 Loss: 0.0052, Intermediate losses: [28.789833984375, 13.110899658203126, 5.94658935546875], Test L2 Loss: 0.0098\n",
      "Epoch 170: Time 3.63s, Train L2 Loss: 0.0058, Intermediate losses: [29.0917919921875, 13.127958984375, 5.94942138671875], Test L2 Loss: 0.0102\n",
      "Epoch 171: Time 3.68s, Train L2 Loss: 0.0057, Intermediate losses: [29.0968994140625, 13.13787353515625, 5.96244384765625], Test L2 Loss: 0.0096\n",
      "Epoch 172: Time 3.57s, Train L2 Loss: 0.0053, Intermediate losses: [29.020234375, 13.107904052734375, 5.967569580078125], Test L2 Loss: 0.0100\n",
      "Epoch 173: Time 3.62s, Train L2 Loss: 0.0054, Intermediate losses: [29.18958740234375, 13.16956787109375, 5.96830078125], Test L2 Loss: 0.0092\n",
      "Epoch 174: Time 3.58s, Train L2 Loss: 0.0052, Intermediate losses: [29.24345703125, 13.18456787109375, 5.97422119140625], Test L2 Loss: 0.0094\n",
      "Epoch 175: Time 3.58s, Train L2 Loss: 0.0055, Intermediate losses: [29.2652392578125, 13.16620849609375, 5.98860107421875], Test L2 Loss: 0.0093\n",
      "Epoch 176: Time 3.68s, Train L2 Loss: 0.0053, Intermediate losses: [29.3184033203125, 13.20450927734375, 5.9967578125], Test L2 Loss: 0.0093\n",
      "Epoch 177: Time 3.54s, Train L2 Loss: 0.0051, Intermediate losses: [29.4314501953125, 13.2431884765625, 5.998670043945313], Test L2 Loss: 0.0095\n",
      "Epoch 178: Time 3.58s, Train L2 Loss: 0.0059, Intermediate losses: [29.58662841796875, 13.23367919921875, 6.0201904296875], Test L2 Loss: 0.0098\n",
      "Epoch 179: Time 3.61s, Train L2 Loss: 0.0054, Intermediate losses: [29.55234619140625, 13.200673828125, 6.020950927734375], Test L2 Loss: 0.0092\n",
      "Epoch 180: Time 3.54s, Train L2 Loss: 0.0052, Intermediate losses: [29.6156005859375, 13.24685302734375, 6.015411376953125], Test L2 Loss: 0.0092\n",
      "Epoch 181: Time 3.54s, Train L2 Loss: 0.0063, Intermediate losses: [29.7295263671875, 13.22636962890625, 6.035685424804687], Test L2 Loss: 0.0097\n",
      "Epoch 182: Time 3.58s, Train L2 Loss: 0.0056, Intermediate losses: [29.7019140625, 13.2645361328125, 6.0633697509765625], Test L2 Loss: 0.0100\n",
      "Epoch 183: Time 3.66s, Train L2 Loss: 0.0052, Intermediate losses: [29.76624267578125, 13.28099609375, 6.057291259765625], Test L2 Loss: 0.0096\n",
      "Epoch 184: Time 3.59s, Train L2 Loss: 0.0049, Intermediate losses: [29.69717041015625, 13.30294189453125, 6.070151977539062], Test L2 Loss: 0.0094\n",
      "Epoch 185: Time 3.60s, Train L2 Loss: 0.0049, Intermediate losses: [29.90529296875, 13.365697021484374, 6.080900268554688], Test L2 Loss: 0.0092\n",
      "Epoch 186: Time 3.66s, Train L2 Loss: 0.0052, Intermediate losses: [30.03099853515625, 13.33897216796875, 6.089186401367187], Test L2 Loss: 0.0093\n",
      "Epoch 187: Time 3.56s, Train L2 Loss: 0.0053, Intermediate losses: [29.9247265625, 13.376512451171875, 6.082620849609375], Test L2 Loss: 0.0098\n",
      "Epoch 188: Time 3.55s, Train L2 Loss: 0.0053, Intermediate losses: [29.94321044921875, 13.38564697265625, 6.1120147705078125], Test L2 Loss: 0.0093\n",
      "Epoch 189: Time 3.54s, Train L2 Loss: 0.0067, Intermediate losses: [30.0439794921875, 13.316669921875, 6.09527587890625], Test L2 Loss: 0.0099\n",
      "Epoch 190: Time 3.62s, Train L2 Loss: 0.0052, Intermediate losses: [30.0733544921875, 13.38639404296875, 6.1094921875], Test L2 Loss: 0.0092\n",
      "Epoch 191: Time 3.64s, Train L2 Loss: 0.0053, Intermediate losses: [30.0629345703125, 13.306500244140626, 6.087933349609375], Test L2 Loss: 0.0100\n",
      "Epoch 192: Time 3.57s, Train L2 Loss: 0.0053, Intermediate losses: [30.1810791015625, 13.39806396484375, 6.1249761962890625], Test L2 Loss: 0.0101\n",
      "Epoch 193: Time 3.58s, Train L2 Loss: 0.0053, Intermediate losses: [30.34890625, 13.49194091796875, 6.1517822265625], Test L2 Loss: 0.0092\n",
      "Epoch 194: Time 3.54s, Train L2 Loss: 0.0052, Intermediate losses: [30.35093994140625, 13.49541015625, 6.165053100585937], Test L2 Loss: 0.0097\n",
      "Epoch 195: Time 3.63s, Train L2 Loss: 0.0055, Intermediate losses: [30.40895263671875, 13.4901416015625, 6.16596923828125], Test L2 Loss: 0.0094\n",
      "Epoch 196: Time 3.62s, Train L2 Loss: 0.0051, Intermediate losses: [30.4169677734375, 13.510067138671875, 6.194370727539063], Test L2 Loss: 0.0097\n",
      "Epoch 197: Time 3.60s, Train L2 Loss: 0.0051, Intermediate losses: [30.5099462890625, 13.55889892578125, 6.164266357421875], Test L2 Loss: 0.0091\n",
      "Epoch 198: Time 3.66s, Train L2 Loss: 0.0053, Intermediate losses: [30.415126953125, 13.5343017578125, 6.192710571289062], Test L2 Loss: 0.0094\n",
      "Epoch 199: Time 3.62s, Train L2 Loss: 0.0053, Intermediate losses: [30.544638671875, 13.54971435546875, 6.18805908203125], Test L2 Loss: 0.0090\n",
      "Epoch 200: Time 3.59s, Train L2 Loss: 0.0045, Intermediate losses: [30.5728857421875, 13.62411376953125, 6.209759521484375], Test L2 Loss: 0.0090\n",
      "Epoch 201: Time 3.57s, Train L2 Loss: 0.0044, Intermediate losses: [30.65403564453125, 13.652672119140625, 6.2043701171875], Test L2 Loss: 0.0088\n",
      "Epoch 202: Time 3.57s, Train L2 Loss: 0.0043, Intermediate losses: [30.683134765625, 13.71850341796875, 6.216982421875], Test L2 Loss: 0.0089\n",
      "Epoch 203: Time 3.59s, Train L2 Loss: 0.0043, Intermediate losses: [30.735380859375, 13.7688525390625, 6.22448974609375], Test L2 Loss: 0.0088\n",
      "Epoch 204: Time 3.57s, Train L2 Loss: 0.0043, Intermediate losses: [30.81343017578125, 13.82375244140625, 6.238536376953125], Test L2 Loss: 0.0088\n",
      "Epoch 205: Time 3.58s, Train L2 Loss: 0.0043, Intermediate losses: [30.8260693359375, 13.88886474609375, 6.247969970703125], Test L2 Loss: 0.0091\n",
      "Epoch 206: Time 3.55s, Train L2 Loss: 0.0043, Intermediate losses: [30.887158203125, 13.90986572265625, 6.2508056640625], Test L2 Loss: 0.0089\n",
      "Epoch 207: Time 3.63s, Train L2 Loss: 0.0042, Intermediate losses: [30.92135009765625, 13.94365234375, 6.263508911132813], Test L2 Loss: 0.0089\n",
      "Epoch 208: Time 3.63s, Train L2 Loss: 0.0042, Intermediate losses: [31.02262939453125, 14.00954345703125, 6.273577880859375], Test L2 Loss: 0.0089\n",
      "Epoch 209: Time 3.60s, Train L2 Loss: 0.0042, Intermediate losses: [31.0986669921875, 14.06647216796875, 6.289553833007813], Test L2 Loss: 0.0088\n",
      "Epoch 210: Time 3.57s, Train L2 Loss: 0.0042, Intermediate losses: [31.101474609375, 14.07866455078125, 6.280858154296875], Test L2 Loss: 0.0090\n",
      "Epoch 211: Time 3.61s, Train L2 Loss: 0.0042, Intermediate losses: [31.1821923828125, 14.155380859375, 6.30006103515625], Test L2 Loss: 0.0089\n",
      "Epoch 212: Time 3.59s, Train L2 Loss: 0.0042, Intermediate losses: [31.2138134765625, 14.18677978515625, 6.310712890625], Test L2 Loss: 0.0089\n",
      "Epoch 213: Time 3.55s, Train L2 Loss: 0.0043, Intermediate losses: [31.252919921875, 14.215489501953124, 6.3129339599609375], Test L2 Loss: 0.0089\n",
      "Epoch 214: Time 3.56s, Train L2 Loss: 0.0043, Intermediate losses: [31.28480712890625, 14.22641845703125, 6.320008544921875], Test L2 Loss: 0.0087\n",
      "Epoch 215: Time 3.64s, Train L2 Loss: 0.0043, Intermediate losses: [31.44789306640625, 14.276591796875, 6.324533081054687], Test L2 Loss: 0.0089\n",
      "Epoch 216: Time 3.65s, Train L2 Loss: 0.0043, Intermediate losses: [31.493955078125, 14.34175048828125, 6.337794189453125], Test L2 Loss: 0.0089\n",
      "Epoch 217: Time 3.69s, Train L2 Loss: 0.0043, Intermediate losses: [31.462626953125, 14.35335693359375, 6.343851318359375], Test L2 Loss: 0.0087\n",
      "Epoch 218: Time 3.58s, Train L2 Loss: 0.0042, Intermediate losses: [31.5651220703125, 14.3898681640625, 6.35023193359375], Test L2 Loss: 0.0088\n",
      "Epoch 219: Time 3.61s, Train L2 Loss: 0.0042, Intermediate losses: [31.6208251953125, 14.4353369140625, 6.3535400390625], Test L2 Loss: 0.0088\n",
      "Epoch 220: Time 3.60s, Train L2 Loss: 0.0043, Intermediate losses: [31.6507861328125, 14.4732861328125, 6.368787841796875], Test L2 Loss: 0.0089\n",
      "Epoch 221: Time 3.70s, Train L2 Loss: 0.0042, Intermediate losses: [31.71484375, 14.5174365234375, 6.3762451171875], Test L2 Loss: 0.0088\n",
      "Epoch 222: Time 3.54s, Train L2 Loss: 0.0043, Intermediate losses: [31.7953955078125, 14.5287646484375, 6.364986572265625], Test L2 Loss: 0.0088\n",
      "Epoch 223: Time 3.60s, Train L2 Loss: 0.0044, Intermediate losses: [31.820087890625, 14.56295654296875, 6.374140625], Test L2 Loss: 0.0089\n",
      "Epoch 224: Time 3.61s, Train L2 Loss: 0.0043, Intermediate losses: [31.8591357421875, 14.576968994140625, 6.38850341796875], Test L2 Loss: 0.0089\n",
      "Epoch 225: Time 3.61s, Train L2 Loss: 0.0042, Intermediate losses: [31.9300830078125, 14.640660400390624, 6.403530883789062], Test L2 Loss: 0.0088\n",
      "Epoch 226: Time 3.59s, Train L2 Loss: 0.0042, Intermediate losses: [31.9257275390625, 14.66490234375, 6.4071826171875], Test L2 Loss: 0.0091\n",
      "Epoch 227: Time 3.68s, Train L2 Loss: 0.0043, Intermediate losses: [32.0405322265625, 14.723482666015625, 6.423652954101563], Test L2 Loss: 0.0089\n",
      "Epoch 228: Time 3.62s, Train L2 Loss: 0.0043, Intermediate losses: [32.1926171875, 14.762064208984375, 6.4372186279296875], Test L2 Loss: 0.0087\n",
      "Epoch 229: Time 3.61s, Train L2 Loss: 0.0042, Intermediate losses: [32.175693359375, 14.7741796875, 6.4189453125], Test L2 Loss: 0.0088\n",
      "Epoch 230: Time 3.61s, Train L2 Loss: 0.0043, Intermediate losses: [32.1780517578125, 14.797078857421875, 6.43983642578125], Test L2 Loss: 0.0088\n",
      "Epoch 231: Time 3.64s, Train L2 Loss: 0.0044, Intermediate losses: [32.2946533203125, 14.84258056640625, 6.453173828125], Test L2 Loss: 0.0089\n",
      "Epoch 232: Time 3.64s, Train L2 Loss: 0.0043, Intermediate losses: [32.3350341796875, 14.870977783203125, 6.45980712890625], Test L2 Loss: 0.0087\n",
      "Epoch 233: Time 3.62s, Train L2 Loss: 0.0042, Intermediate losses: [32.3288134765625, 14.90546142578125, 6.469630126953125], Test L2 Loss: 0.0088\n",
      "Epoch 234: Time 3.63s, Train L2 Loss: 0.0043, Intermediate losses: [32.44953857421875, 14.947364501953125, 6.46822509765625], Test L2 Loss: 0.0087\n",
      "Epoch 235: Time 3.62s, Train L2 Loss: 0.0042, Intermediate losses: [32.4883935546875, 14.979722900390625, 6.480693359375], Test L2 Loss: 0.0088\n",
      "Epoch 236: Time 3.62s, Train L2 Loss: 0.0042, Intermediate losses: [32.5428125, 15.00358642578125, 6.482604370117188], Test L2 Loss: 0.0087\n",
      "Epoch 237: Time 3.62s, Train L2 Loss: 0.0041, Intermediate losses: [32.612734375, 15.06240966796875, 6.49159912109375], Test L2 Loss: 0.0088\n",
      "Epoch 238: Time 3.59s, Train L2 Loss: 0.0042, Intermediate losses: [32.6988232421875, 15.10121826171875, 6.510022583007813], Test L2 Loss: 0.0087\n",
      "Epoch 239: Time 3.62s, Train L2 Loss: 0.0044, Intermediate losses: [32.7301220703125, 15.10597900390625, 6.508074340820312], Test L2 Loss: 0.0089\n",
      "Epoch 240: Time 3.63s, Train L2 Loss: 0.0043, Intermediate losses: [32.80885009765625, 15.15420654296875, 6.517030029296875], Test L2 Loss: 0.0088\n",
      "Epoch 241: Time 3.62s, Train L2 Loss: 0.0044, Intermediate losses: [32.813154296875, 15.14, 6.515465087890625], Test L2 Loss: 0.0090\n",
      "Epoch 242: Time 3.59s, Train L2 Loss: 0.0045, Intermediate losses: [32.8584228515625, 15.1180322265625, 6.516485595703125], Test L2 Loss: 0.0088\n",
      "Epoch 243: Time 3.62s, Train L2 Loss: 0.0042, Intermediate losses: [32.95255859375, 15.16668212890625, 6.524243774414063], Test L2 Loss: 0.0089\n",
      "Epoch 244: Time 3.56s, Train L2 Loss: 0.0043, Intermediate losses: [33.013876953125, 15.211256103515625, 6.548028564453125], Test L2 Loss: 0.0087\n",
      "Epoch 245: Time 3.60s, Train L2 Loss: 0.0042, Intermediate losses: [33.0708642578125, 15.2586083984375, 6.550694580078125], Test L2 Loss: 0.0089\n",
      "Epoch 246: Time 3.59s, Train L2 Loss: 0.0043, Intermediate losses: [33.14184326171875, 15.251903076171875, 6.54101318359375], Test L2 Loss: 0.0087\n",
      "Epoch 247: Time 3.63s, Train L2 Loss: 0.0044, Intermediate losses: [33.13626953125, 15.266700439453125, 6.56448974609375], Test L2 Loss: 0.0087\n",
      "Epoch 248: Time 3.62s, Train L2 Loss: 0.0042, Intermediate losses: [33.200947265625, 15.312099609375, 6.563304443359375], Test L2 Loss: 0.0088\n",
      "Epoch 249: Time 3.59s, Train L2 Loss: 0.0041, Intermediate losses: [33.26346435546875, 15.35161865234375, 6.575113525390625], Test L2 Loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "model = FNO2d(modes, modes, width).cuda()\n",
    "print(count_params(model))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, _ = model(x)\n",
    "        out = out.reshape(batch_size, s, s)\n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            out, out_list = model(x)\n",
    "            out = out.reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "            for out in out_list:\n",
    "                out = out.reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "            test_l2_list = [myloss(o.view(batch_size, -1), y.view(batch_size,-1)).item() for o in out_list]\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "    test_l2_list = [tl / ntest for tl in test_l2_list]\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(f'Epoch {ep}: Time {t2-t1:.2f}s, Train L2 Loss: {train_l2:.4f}, Intermediate losses: {test_l2_list}, Test L2 Loss: {test_l2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a9d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
