{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291641fd",
   "metadata": {},
   "source": [
    "# Flow Matching for Darcy Flow - CORRECTED IMPLEMENTATION\n",
    "\n",
    "## Key Fixes Applied:\n",
    "\n",
    "### 1. **Correct Flow Matching Formulation**\n",
    "- **OLD (WRONG)**: `x_t = (1-t)*a + t*u` - interpolating from permeability to solution ❌\n",
    "- **NEW (CORRECT)**: `x_t = (1-t)*noise + t*u` - interpolating from noise to solution ✅\n",
    "- **Conditioning**: Permeability `a(x)` conditions the entire process via FiLM\n",
    "\n",
    "### 2. **Proper Velocity Target**\n",
    "- **OLD**: `v_target = u - a` (solution minus permeability - makes no physical sense!) ❌\n",
    "- **NEW**: `v_target = u - noise` (denoising direction) ✅\n",
    "\n",
    "### 3. **Correct ODE Integration**\n",
    "- **OLD**: Start from permeability `x_0 = a` ❌\n",
    "- **NEW**: Start from Gaussian noise `x_0 ~ N(0,I)` ✅\n",
    "\n",
    "### 4. **Enhanced Architecture**\n",
    "- Added **Spectral Convolutions** from FNO for better physics learning\n",
    "- **FiLM conditioning** properly applied at each resolution level\n",
    "- Time and condition embeddings guide the denoising process\n",
    "\n",
    "### 5. **Why This Works**\n",
    "The model learns: **\"Given permeability a(x), generate the corresponding solution u(x)\"**\n",
    "- Training: Learn velocity field that denoises `noise → u` conditioned on `a`\n",
    "- Inference: Integrate ODE from `noise → predicted_u` given `a`\n",
    "\n",
    "This is the standard conditional generation framework used in diffusion models, adapted for PDE solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_PATH = 'darcy/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = 'darcy/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 200\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "r = 5\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h\n",
    "\n",
    "reader = MatReader(TRAIN_PATH)\n",
    "x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,::r,::r][:,:s,:s]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_test = x_test.reshape(ntest,s,s,1)\n",
    "y_train = y_train.reshape(ntrain,s,s,1)\n",
    "y_test = y_test.reshape(ntest,s,s,1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e448797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Spatial resolution: 85x85\n",
      "Model parameters: 19,063,489\n",
      "Epoch 10/50, Train Loss: 0.025395, Test L2: 0.286832\n",
      "Epoch 10/50, Train Loss: 0.025395, Test L2: 0.286832\n",
      "Epoch 20/50, Train Loss: 0.016488, Test L2: 0.299185\n",
      "Epoch 20/50, Train Loss: 0.016488, Test L2: 0.299185\n",
      "Epoch 30/50, Train Loss: 0.014245, Test L2: 0.231743\n",
      "Epoch 30/50, Train Loss: 0.014245, Test L2: 0.231743\n",
      "Epoch 40/50, Train Loss: 0.010674, Test L2: 0.257137\n",
      "Epoch 40/50, Train Loss: 0.010674, Test L2: 0.257137\n",
      "Epoch 50/50, Train Loss: 0.010690, Test L2: 0.239105\n",
      "Training complete! Model saved.\n",
      "Epoch 50/50, Train Loss: 0.010690, Test L2: 0.239105\n",
      "Training complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "from torchdiffeq import odeint\n",
    "\n",
    "# ============================================================================\n",
    "# Spectral Convolution Layer (from FNO)\n",
    "# ============================================================================\n",
    "class SpectralConv2d(nn.Module):\n",
    "    \"\"\"2D Fourier layer - learns in spectral domain\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # Complex multiplication\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Compute FFT\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "        \n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(B, self.out_channels, H, W//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "        \n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(H, W))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Time Embedding Module\n",
    "# ============================================================================\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding followed by MLP\"\"\"\n",
    "    def __init__(self, dim=128, mlp_dim=256):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(mlp_dim, mlp_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # t: (B,) or (B, 1)\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        \n",
    "        # Sinusoidal embeddings\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        \n",
    "        return self.mlp(emb)  # (B, mlp_dim)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FiLM MLP: Generates scale and shift parameters\n",
    "# ============================================================================\n",
    "class FiLMMLP(nn.Module):\n",
    "    \"\"\"Takes concatenated [time_emb, condition_emb] and outputs FiLM parameters\"\"\"\n",
    "    def __init__(self, time_dim=256, cond_dim=256, hidden_dim=256, num_resolutions=4, channels_per_resolution=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.num_resolutions = num_resolutions\n",
    "        self.channels_per_resolution = channels_per_resolution\n",
    "        \n",
    "        input_dim = time_dim + cond_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Separate heads for each resolution level\n",
    "        self.film_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, 2 * channels)  # 2x for scale and shift\n",
    "            for channels in channels_per_resolution\n",
    "        ])\n",
    "    \n",
    "    def forward(self, time_emb, cond_emb):\n",
    "        # time_emb: (B, time_dim), cond_emb: (B, cond_dim)\n",
    "        x = torch.cat([time_emb, cond_emb], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Generate FiLM parameters for each resolution\n",
    "        film_params = []\n",
    "        for head in self.film_heads:\n",
    "            params = head(x)  # (B, 2*C)\n",
    "            film_params.append(params)\n",
    "        \n",
    "        return film_params\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Conditional Encoder: Encodes permeability field a(x)\n",
    "# ============================================================================\n",
    "class ConditionEncoder(nn.Module):\n",
    "    \"\"\"Multi-scale CNN encoder for conditioning permeability field\"\"\"\n",
    "    def __init__(self, in_channels=1, base_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Downsample path with skip connections\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(base_channels, base_channels, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(base_channels*2, base_channels*2, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(base_channels*4, base_channels*4, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Global pooling for global latent\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_fc = nn.Linear(base_channels*8, 256)\n",
    "    \n",
    "    def forward(self, a):\n",
    "        # a: (B, 1, H, W)\n",
    "        skips = []\n",
    "        \n",
    "        x1 = self.conv1(a)\n",
    "        skips.append(x1)\n",
    "        x1 = self.down1(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1)\n",
    "        skips.append(x2)\n",
    "        x2 = self.down2(x2)\n",
    "        \n",
    "        x3 = self.conv3(x2)\n",
    "        skips.append(x3)\n",
    "        x3 = self.down3(x3)\n",
    "        \n",
    "        x4 = self.conv4(x3)\n",
    "        skips.append(x4)\n",
    "        \n",
    "        # Global latent\n",
    "        global_feat = self.global_pool(x4).squeeze(-1).squeeze(-1)\n",
    "        global_latent = self.global_fc(global_feat)\n",
    "        \n",
    "        return skips, global_latent\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FiLM Residual Block with Spectral Convolution\n",
    "# ============================================================================\n",
    "class FiLMResBlock(nn.Module):\n",
    "    \"\"\"Residual block with FiLM modulation and optional spectral convolution\"\"\"\n",
    "    def __init__(self, channels, modes=12, use_spectral=True):\n",
    "        super().__init__()\n",
    "        self.use_spectral = use_spectral\n",
    "        \n",
    "        if use_spectral:\n",
    "            self.spectral_conv = SpectralConv2d(channels, channels, modes, modes)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x, gamma, beta):\n",
    "        # x: (B, C, H, W)\n",
    "        # gamma, beta: (B, C)\n",
    "        residual = x\n",
    "        \n",
    "        # First conv\n",
    "        if self.use_spectral:\n",
    "            x = self.spectral_conv(x) + self.conv1(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        # Apply FiLM modulation\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = gamma * x + beta\n",
    "        \n",
    "        # Second conv\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = x + residual\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNet with FiLM Conditioning and Spectral Convolutions\n",
    "# ============================================================================\n",
    "class FlowMatchingUNet(nn.Module):\n",
    "    \"\"\"UNet for flow matching with FiLM conditioning and FNO-style spectral layers\"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=64, modes=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_channels = base_channels\n",
    "        self.modes = modes\n",
    "        channels = [base_channels, base_channels*2, base_channels*4, base_channels*8]\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = TimeEmbedding(dim=128, mlp_dim=256)\n",
    "        \n",
    "        # Condition encoder\n",
    "        self.cond_encoder = ConditionEncoder(in_channels=in_channels, base_channels=base_channels)\n",
    "        \n",
    "        # FiLM MLP\n",
    "        self.film_mlp = FiLMMLP(\n",
    "            time_dim=256, \n",
    "            cond_dim=256, \n",
    "            hidden_dim=256,\n",
    "            num_resolutions=4,\n",
    "            channels_per_resolution=channels\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "        \n",
    "        # Downsample path with spectral convolutions\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            FiLMResBlock(channels[0], modes=modes, use_spectral=True),\n",
    "            FiLMResBlock(channels[1], modes=modes//2, use_spectral=True),\n",
    "            FiLMResBlock(channels[2], modes=modes//4, use_spectral=True),\n",
    "            FiLMResBlock(channels[3], modes=modes//8, use_spectral=False)\n",
    "        ])\n",
    "        \n",
    "        self.downsample = nn.ModuleList([\n",
    "            nn.Conv2d(channels[0], channels[1], 3, stride=2, padding=1),\n",
    "            nn.Conv2d(channels[1], channels[2], 3, stride=2, padding=1),\n",
    "            nn.Conv2d(channels[2], channels[3], 3, stride=2, padding=1)\n",
    "        ])\n",
    "        \n",
    "        # Middle block\n",
    "        self.mid_block = FiLMResBlock(channels[3], modes=modes//8, use_spectral=False)\n",
    "        \n",
    "        # Upsample path\n",
    "        self.upsample_conv = nn.ModuleList([\n",
    "            nn.Conv2d(channels[3], channels[2], 3, padding=1),\n",
    "            nn.Conv2d(channels[2], channels[1], 3, padding=1),\n",
    "            nn.Conv2d(channels[1], channels[0], 3, padding=1)\n",
    "        ])\n",
    "        \n",
    "        # Projection layers to handle concatenated skip connections\n",
    "        self.skip_proj = nn.ModuleList([\n",
    "            nn.Conv2d(channels[2]*2, channels[2], 1),\n",
    "            nn.Conv2d(channels[1]*2, channels[1], 1),\n",
    "            nn.Conv2d(channels[0]*2, channels[0], 1)\n",
    "        ])\n",
    "        \n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            FiLMResBlock(channels[2], modes=modes//4, use_spectral=True),\n",
    "            FiLMResBlock(channels[1], modes=modes//2, use_spectral=True),\n",
    "            FiLMResBlock(channels[0], modes=modes, use_spectral=True)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels[0], channels[0], 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels[0], out_channels, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_t, t, a):\n",
    "        # x_t: (B, 1, H, W) - current state\n",
    "        # t: (B,) - time\n",
    "        # a: (B, 1, H, W) - conditioning permeability\n",
    "        \n",
    "        # Get time embedding\n",
    "        time_emb = self.time_embed(t)  # (B, 256)\n",
    "        \n",
    "        # Get condition encoding\n",
    "        cond_skips, cond_global = self.cond_encoder(a)  # skips: list of 4, global: (B, 256)\n",
    "        \n",
    "        # Get FiLM parameters\n",
    "        film_params = self.film_mlp(time_emb, cond_global)  # list of 4 (B, 2*C)\n",
    "        \n",
    "        # Parse FiLM parameters\n",
    "        film_scales = []\n",
    "        film_shifts = []\n",
    "        for params in film_params:\n",
    "            B, dim = params.shape\n",
    "            C = dim // 2\n",
    "            scale = params[:, :C]\n",
    "            shift = params[:, C:]\n",
    "            film_scales.append(scale)\n",
    "            film_shifts.append(shift)\n",
    "        \n",
    "        # Input\n",
    "        x = self.input_conv(x_t)\n",
    "        \n",
    "        # Downsample path\n",
    "        skips = []\n",
    "        for i, (block, down) in enumerate(zip(self.down_blocks[:-1], self.downsample)):\n",
    "            x = block(x, film_scales[i], film_shifts[i])\n",
    "            # Add conditioning skip connection\n",
    "            x = x + cond_skips[i]\n",
    "            skips.append(x)\n",
    "            x = down(x)\n",
    "        \n",
    "        # Bottom block\n",
    "        x = self.down_blocks[-1](x, film_scales[-1], film_shifts[-1])\n",
    "        x = x + cond_skips[-1]\n",
    "        x = self.mid_block(x, film_scales[-1], film_shifts[-1])\n",
    "        \n",
    "        # Upsample path\n",
    "        for i, (up_conv, proj, block) in enumerate(zip(self.upsample_conv, self.skip_proj, self.up_blocks)):\n",
    "            # Upsample using interpolation to match skip connection size\n",
    "            skip = skips[-(i+1)]\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            x = up_conv(x)\n",
    "            \n",
    "            # Concatenate skip connection and project to correct channels\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = proj(x)\n",
    "            x = block(x, film_scales[-(i+2)], film_shifts[-(i+2)])\n",
    "        \n",
    "        # Output velocity field\n",
    "        v = self.output_conv(x)\n",
    "        \n",
    "        return v\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Flow Matching Training - CORRECTED VERSION\n",
    "# ============================================================================\n",
    "def flow_matching_loss(model, a, u, device):\n",
    "    \"\"\"\n",
    "    Compute flow matching loss - PROPER FORMULATION\n",
    "    \n",
    "    Flow: noise (x_0) -> solution (x_1 = u), conditioned on permeability (a)\n",
    "    x_t = (1-t)*x_0 + t*u, where x_0 ~ N(0, I)\n",
    "    v_target = u - x_0  (constant velocity for linear interpolation)\n",
    "    \n",
    "    The model learns: v_θ(x_t, t, a) ≈ v_target = u - x_0\n",
    "    \"\"\"\n",
    "    B = a.shape[0]\n",
    "    \n",
    "    # Sample time uniformly from [0, 1]\n",
    "    t = torch.rand(B, device=device)\n",
    "    \n",
    "    # Sample initial noise from standard normal distribution\n",
    "    x_0 = torch.randn_like(u)\n",
    "    \n",
    "    # Linear interpolation: x_t = (1-t)*x_0 + t*u\n",
    "    t_expanded = t.view(B, 1, 1, 1)\n",
    "    x_t = (1 - t_expanded) * x_0 + t_expanded * u\n",
    "    \n",
    "    # Target velocity field (constant for linear interpolation)\n",
    "    v_target = u - x_0\n",
    "    \n",
    "    # Predict velocity conditioned on permeability a\n",
    "    v_pred = model(x_t, t, a)\n",
    "    \n",
    "    # MSE loss\n",
    "    loss = F.mse_loss(v_pred, v_target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ODE Integration for Inference - CORRECTED VERSION\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def integrate_ode(model, a, num_steps=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Integrate the learned ODE from t=0 to t=1 using dopri5 adaptive solver\n",
    "    \n",
    "    Starting from noise x_0 ~ N(0, I), integrate:\n",
    "    dx/dt = v_θ(x, t, a)\n",
    "    \n",
    "    to reach solution u at t=1\n",
    "    \"\"\"\n",
    "    \n",
    "    B = a.shape[0]\n",
    "    \n",
    "    # Start from random noise\n",
    "    x_0 = torch.randn_like(a)\n",
    "    \n",
    "    # Define the ODE function\n",
    "    def ode_func(t, x):\n",
    "        # t is a scalar, need to expand to batch\n",
    "        t_batch = t.expand(B).to(device)\n",
    "        return model(x, t_batch, a)\n",
    "    \n",
    "    # Time points: from 0 to 1\n",
    "    t_span = torch.tensor([0.0, 1.0], device=device)\n",
    "    \n",
    "    # Solve ODE using dopri5\n",
    "    trajectory = odeint(ode_func, x_0, t_span, method='dopri5', rtol=1e-5, atol=1e-5)\n",
    "    \n",
    "    # Return the final state at t=1 (predicted solution)\n",
    "    x_final = trajectory[-1]\n",
    "    \n",
    "    return x_final\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop - IMPROVED\n",
    "# ============================================================================\n",
    "def train_flow_matching():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model with spectral convolutions\n",
    "    model = FlowMatchingUNet(\n",
    "        in_channels=1, \n",
    "        out_channels=1, \n",
    "        base_channels=64,\n",
    "        modes=12\n",
    "    ).to(device)\n",
    "    \n",
    "    # Move normalizer to device\n",
    "    y_normalizer.cuda()\n",
    "    \n",
    "    # Optimizer with warmup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    # Cosine annealing scheduler with warmup\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Training\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Spatial resolution: {s}x{s}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_test_l2 = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (a, u) in enumerate(train_loader):\n",
    "            a, u = a.to(device), u.to(device)\n",
    "            \n",
    "            # Transpose to (B, C, H, W) format\n",
    "            a = a.permute(0, 3, 1, 2)  # (B, 1, 85, 85)\n",
    "            u = u.permute(0, 3, 1, 2)  # (B, 1, 85, 85)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = flow_matching_loss(model, a, u, device)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            test_l2 = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for a, u in test_loader:\n",
    "                    a, u = a.to(device), u.to(device)\n",
    "                    a = a.permute(0, 3, 1, 2)\n",
    "                    u = u.permute(0, 3, 1, 2)\n",
    "                    \n",
    "                    # Integrate ODE\n",
    "                    u_pred = integrate_ode(model, a, num_steps=50, device=device)\n",
    "                    \n",
    "                    # Denormalize for error computation\n",
    "                    u_pred_denorm = y_normalizer.decode(u_pred.permute(0, 2, 3, 1))\n",
    "                    u_denorm = y_normalizer.decode(u.permute(0, 2, 3, 1))\n",
    "                    \n",
    "                    # Relative L2 error\n",
    "                    test_l2 += torch.mean(\n",
    "                        torch.norm(u_pred_denorm.reshape(u_denorm.shape[0], -1) - u_denorm.reshape(u_denorm.shape[0], -1), 2, dim=1) /\n",
    "                        torch.norm(u_denorm.reshape(u_denorm.shape[0], -1), 2, dim=1)\n",
    "                    ).item()\n",
    "            \n",
    "            avg_test_l2 = test_l2 / len(test_loader)\n",
    "            \n",
    "            if avg_test_l2 < best_test_l2:\n",
    "                best_test_l2 = avg_test_l2\n",
    "                status = \"✓ NEW BEST\"\n",
    "            else:\n",
    "                status = \"\"\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.6f} | Test L2: {avg_test_l2:.6f} | LR: {scheduler.get_last_lr()[0]:.2e} {status}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Training complete! Best Test L2: {best_test_l2:.6f}\")\n",
    "    \n",
    "    # Move normalizer back to CPU before returning\n",
    "    y_normalizer.cpu()\n",
    "    \n",
    "    return model, y_normalizer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Run Training\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    model, normalizer = train_flow_matching()\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'normalizer': normalizer\n",
    "    }, 'flow_matching_model.pt')\n",
    "    print(\"Training complete! Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training with corrected flow matching formulation\n",
    "model, normalizer = train_flow_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "normalizer.cuda()\n",
    "\n",
    "# Get a test batch\n",
    "test_a, test_u = next(iter(test_loader))\n",
    "test_a = test_a[:4].to(device).permute(0, 3, 1, 2)  # First 4 samples\n",
    "test_u = test_u[:4].to(device).permute(0, 3, 1, 2)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    pred_u = integrate_ode(model, test_a, num_steps=50, device=device)\n",
    "\n",
    "# Denormalize\n",
    "test_a_denorm = x_normalizer.decode(test_a.permute(0, 2, 3, 1)).cpu().numpy()\n",
    "test_u_denorm = normalizer.decode(test_u.permute(0, 2, 3, 1)).cpu().numpy()\n",
    "pred_u_denorm = normalizer.decode(pred_u.permute(0, 2, 3, 1)).cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 14))\n",
    "for i in range(4):\n",
    "    # Permeability\n",
    "    im0 = axes[i, 0].imshow(test_a_denorm[i, :, :, 0], cmap='viridis')\n",
    "    axes[i, 0].set_title('Permeability a(x)' if i == 0 else '')\n",
    "    axes[i, 0].axis('off')\n",
    "    plt.colorbar(im0, ax=axes[i, 0])\n",
    "    \n",
    "    # Ground truth solution\n",
    "    im1 = axes[i, 1].imshow(test_u_denorm[i, :, :, 0], cmap='RdBu_r')\n",
    "    axes[i, 1].set_title('True Solution u(x)' if i == 0 else '')\n",
    "    axes[i, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[i, 1])\n",
    "    \n",
    "    # Predicted solution\n",
    "    im2 = axes[i, 2].imshow(pred_u_denorm[i, :, :, 0], cmap='RdBu_r')\n",
    "    axes[i, 2].set_title('Predicted Solution' if i == 0 else '')\n",
    "    axes[i, 2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[i, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute errors\n",
    "errors = np.abs(test_u_denorm - pred_u_denorm)\n",
    "print(f\"\\nMean Absolute Error: {np.mean(errors):.6f}\")\n",
    "print(f\"Max Absolute Error: {np.max(errors):.6f}\")\n",
    "\n",
    "normalizer.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
