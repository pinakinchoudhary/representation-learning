{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b65050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pinakinchoudhary/dev/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training VQ-VAE on Darcy Flow Dataset\n",
      "Codebook size: 256, Latent dim: 32\n",
      "\n",
      "Model Parameters: 870,753\n",
      "\n",
      "Starting training...\n",
      "Epoch 10/200:\n",
      "  Train Loss: 0.999541, VQ Loss: 0.000001\n",
      "  Test Loss: 0.997808\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000300\n",
      "Epoch 20/200:\n",
      "  Train Loss: 0.999508, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997841\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000150\n",
      "Epoch 30/200:\n",
      "  Train Loss: 0.999405, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997869\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000075\n",
      "Epoch 40/200:\n",
      "  Train Loss: 0.999409, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997879\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000037\n",
      "Epoch 50/200:\n",
      "  Train Loss: 0.999419, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997885\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000019\n",
      "Epoch 60/200:\n",
      "  Train Loss: 0.999406, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997887\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000009\n",
      "Epoch 70/200:\n",
      "  Train Loss: 0.999448, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997888\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000005\n",
      "Epoch 80/200:\n",
      "  Train Loss: 0.999428, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997889\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000002\n",
      "Epoch 90/200:\n",
      "  Train Loss: 0.999431, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997890\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000001\n",
      "Epoch 100/200:\n",
      "  Train Loss: 0.999420, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997890\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000001\n",
      "Epoch 110/200:\n",
      "  Train Loss: 0.999415, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997890\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000001\n",
      "Epoch 120/200:\n",
      "  Train Loss: 0.999387, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997891\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000000\n",
      "Epoch 130/200:\n",
      "  Train Loss: 0.999501, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997891\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000000\n",
      "Epoch 140/200:\n",
      "  Train Loss: 0.999315, VQ Loss: 0.000000\n",
      "  Test Loss: 0.997891\n",
      "  Perplexity: 1.00 (codebook usage)\n",
      "  LR: 0.000000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 379\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    378\u001b[0m t0 \u001b[38;5;241m=\u001b[39m default_timer()\n\u001b[0;32m--> 379\u001b[0m train_losses, test_losses, perplexities \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vqvae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m t1 \u001b[38;5;241m=\u001b[39m default_timer()\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 219\u001b[0m, in \u001b[0;36mtrain_vqvae\u001b[0;34m(model, train_loader, test_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m    216\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    218\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 219\u001b[0m recon, vq_loss, perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Reconstruction loss\u001b[39;00m\n\u001b[1;32m    222\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(recon, data)\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 171\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m    170\u001b[0m quantized, vq_loss, perplexity, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvq(z)\n\u001b[0;32m--> 171\u001b[0m recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Crop or pad to match input size\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]:\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 153\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 153\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Handle size mismatch if needed\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 99\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/myenv/lib/python3.10/site-packages/torch/nn/functional.py:1701\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1699\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1701\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============================================================================\n",
    "# VQ-VAE Components\n",
    "# ============================================================================\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantization layer with exponential moving average updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25, \n",
    "                 decay=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Codebook with EMA\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.normal_()  # Better initialization\n",
    "        \n",
    "        # EMA tracking\n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('_ema_w', self.embedding.weight.data.clone())\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # z: [B, C, H, W]\n",
    "        # Reshape to [B*H*W, C]\n",
    "        z_flattened = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z_flattened.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # Calculate distances to codebook vectors\n",
    "        # |z - e|^2 = |z|^2 + |e|^2 - 2*z*e\n",
    "        distances = (torch.sum(z_flattened**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(z_flattened, self.embedding.weight.t()))\n",
    "        \n",
    "        # Find nearest codebook entry\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=z.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight)\n",
    "        quantized = quantized.view(z.shape[0], z.shape[2], z.shape[3], self.embedding_dim)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # EMA codebook update (only during training)\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self.decay + \\\n",
    "                                     (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "            \n",
    "            n = torch.sum(self._ema_cluster_size)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self.epsilon) /\n",
    "                (n + self.num_embeddings * self.epsilon) * n\n",
    "            )\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), z_flattened)\n",
    "            self._ema_w = self._ema_w * self.decay + (1 - self.decay) * dw\n",
    "            \n",
    "            self.embedding.weight.data = self._ema_w / self._ema_cluster_size.unsqueeze(1)\n",
    "        \n",
    "        # Loss - only commitment loss with EMA\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), z)\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        quantized = z + (quantized - z).detach()\n",
    "        \n",
    "        # Perplexity (measure of codebook usage)\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        return quantized, loss, perplexity, encoding_indices\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: 85x85 -> 21x21 (or 22x22 depending on architecture)\n",
    "    Uses strided convolutions for downsampling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, hidden_dims=[32, 64, 128], latent_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = in_channels\n",
    "        \n",
    "        # Downsampling layers\n",
    "        # 85 -> 43 -> 22 -> 11 or similar\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Conv2d(prev_dim, h_dim, 4, stride=2, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(ResidualBlock(h_dim))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        # Final projection to latent space\n",
    "        layers.append(nn.Conv2d(prev_dim, latent_dim, 3, padding=1))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: upsamples back to 85x85\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64, hidden_dims=[128, 64, 32], out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = latent_dim\n",
    "        \n",
    "        # Upsampling layers\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.ConvTranspose2d(prev_dim, h_dim, 4, stride=2, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(ResidualBlock(h_dim))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        # Final layer - may need adjustment to get exact 85x85\n",
    "        layers.append(nn.ConvTranspose2d(prev_dim, out_channels, 4, stride=2, padding=1))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.decoder(x)\n",
    "        # Handle size mismatch if needed\n",
    "        return out\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_dims=[32, 64, 128], \n",
    "                 latent_dim=64, num_embeddings=512, commitment_cost=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_channels, hidden_dims, latent_dim)\n",
    "        self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost, \n",
    "                                  decay=0.99, epsilon=1e-5)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dims[::-1], in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, perplexity, indices = self.vq(z)\n",
    "        recon = self.decoder(quantized)\n",
    "        \n",
    "        # Crop or pad to match input size\n",
    "        if recon.shape[-2:] != x.shape[-2:]:\n",
    "            recon = F.interpolate(recon, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return recon, vq_loss, perplexity\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        _, _, _, indices = self.vq(z)\n",
    "        return indices\n",
    "    \n",
    "    def decode_indices(self, indices, spatial_shape):\n",
    "        # Reconstruct from indices\n",
    "        quantized = self.vq.embedding(indices)\n",
    "        quantized = quantized.view(indices.shape[0], spatial_shape[0], spatial_shape[1], -1)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "        recon = self.decoder(quantized)\n",
    "        if recon.shape[-2:] != (85, 85):\n",
    "            recon = F.interpolate(recon, size=(85, 85), mode='bilinear', align_corners=False)\n",
    "        return recon\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_vqvae(model, train_loader, test_loader, epochs=100, lr=1e-3, device='cuda'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                           factor=0.5, patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    perplexities = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_vq_loss = 0\n",
    "        train_perplexity = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, vq_loss, perplexity = model(data)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            recon_loss = F.mse_loss(recon, data)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = recon_loss + vq_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += recon_loss.item()\n",
    "            train_vq_loss += vq_loss.item()\n",
    "            train_perplexity += perplexity.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_vq_loss /= len(train_loader)\n",
    "        train_perplexity /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        perplexities.append(train_perplexity)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                recon, vq_loss, _ = model(data)\n",
    "                recon_loss = F.mse_loss(recon, data)\n",
    "                test_loss += (recon_loss + vq_loss).item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.6f}, VQ Loss: {train_vq_loss:.6f}')\n",
    "            print(f'  Test Loss: {test_loss:.6f}')\n",
    "            print(f'  Perplexity: {train_perplexity:.2f} (codebook usage)')\n",
    "            print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    return train_losses, test_losses, perplexities\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_reconstruction(model, test_data, num_samples=5, device='cuda'):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            data = test_data[i:i+1].to(device)\n",
    "            recon, _, _ = model(data)\n",
    "            \n",
    "            # Original\n",
    "            axes[0, i].imshow(data[0, 0].cpu().numpy(), cmap='viridis')\n",
    "            axes[0, i].set_title(f'Original {i+1}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstruction\n",
    "            axes[1, i].imshow(recon[0, 0].cpu().numpy(), cmap='viridis')\n",
    "            axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vqvae_reconstruction.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses, perplexities):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(train_losses, label='Train')\n",
    "    axes[0].plot(test_losses, label='Test')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Reconstruction Loss')\n",
    "    axes[0].set_title('Training Progress')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].semilogy(train_losses, label='Train')\n",
    "    axes[1].semilogy(test_losses, label='Test')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Log Loss')\n",
    "    axes[1].set_title('Training Progress (Log Scale)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(perplexities)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Perplexity')\n",
    "    axes[2].set_title('Codebook Usage')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vqvae_training.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Training Script\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 200  # More epochs with EMA\n",
    "    LR = 3e-4  # Lower learning rate\n",
    "    LATENT_DIM = 32  # Smaller latent space to force compression\n",
    "    NUM_EMBEDDINGS = 256  # Smaller codebook - easier to use\n",
    "    COMMITMENT_COST = 1.0  # Higher commitment - prevent encoder from drifting\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Training VQ-VAE on Darcy Flow Dataset\")\n",
    "    print(f\"Codebook size: {NUM_EMBEDDINGS}, Latent dim: {LATENT_DIM}\")\n",
    "    \n",
    "    # NOTE: You need to load your data here\n",
    "    # This assumes x_train, x_test are already loaded and normalized\n",
    "    # Shape should be [N, 85, 85, 1] or [N, 1, 85, 85]\n",
    "    \n",
    "    # Example data loading (replace with your actual data):\n",
    "    # x_train = torch.from_numpy(x_train).float()  # [1000, 85, 85, 1]\n",
    "    # x_test = torch.from_numpy(x_test).float()\n",
    "    \n",
    "    # Ensure channel-first format [N, C, H, W]\n",
    "    # if x_train.shape[-1] == 1:\n",
    "    #     x_train = x_train.permute(0, 3, 1, 2)\n",
    "    #     x_test = x_test.permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Create dummy data for demonstration\n",
    "    x_train = torch.randn(1000, 1, 85, 85)\n",
    "    x_test = torch.randn(200, 1, 85, 85)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(x_train, torch.zeros(len(x_train)))\n",
    "    test_dataset = TensorDataset(x_test, torch.zeros(len(x_test)))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VQVAE(\n",
    "        in_channels=1,\n",
    "        hidden_dims=[32, 64, 128],\n",
    "        latent_dim=LATENT_DIM,\n",
    "        num_embeddings=NUM_EMBEDDINGS,\n",
    "        commitment_cost=COMMITMENT_COST\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"\\nModel Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    t0 = default_timer()\n",
    "    train_losses, test_losses, perplexities = train_vqvae(\n",
    "        model, train_loader, test_loader, \n",
    "        epochs=EPOCHS, lr=LR, device=DEVICE\n",
    "    )\n",
    "    t1 = default_timer()\n",
    "    print(f\"\\nTraining completed in {t1-t0:.2f} seconds\")\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_reconstruction(model, x_test, num_samples=5, device=DEVICE)\n",
    "    plot_training_curves(train_losses, test_losses, perplexities)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'vqvae_darcy.pth')\n",
    "    print(\"\\nModel saved to 'vqvae_darcy.pth'\")\n",
    "    print(\"Visualizations saved to 'vqvae_reconstruction.png' and 'vqvae_training.png'\")\n",
    "    \n",
    "    # Final metrics\n",
    "    print(f\"\\nFinal Test Loss: {test_losses[-1]:.6f}\")\n",
    "    print(f\"Final Perplexity: {perplexities[-1]:.2f} / {NUM_EMBEDDINGS}\")\n",
    "    print(f\"Codebook usage: {perplexities[-1]/NUM_EMBEDDINGS*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
