{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e667281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298edc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'darcy/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = 'darcy/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 10\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "r = 5\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MatReader(TRAIN_PATH)\n",
    "x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,::r,::r][:,:s,:s]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_test = x_test.reshape(ntest,s,s,1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88df07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8344efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_stats = []\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x_prev = x[..., :-self.padding, :-self.padding].detach().clone()\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        x_curr = x[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        if not self.training:\n",
    "            residual = x_curr - x_prev\n",
    "            intermediate_stats.append({\n",
    "                'layer': 0,\n",
    "                'residual_norm': residual.norm(dim=1).mean().item(),\n",
    "                'relative_change': (residual.norm(dim=1) / (x_prev.norm(dim=1) + 1e-8)).mean().item(),\n",
    "                'feature_norm': x_curr.norm(dim=1).mean().item()\n",
    "            })\n",
    "\n",
    "        x_prev = x[..., :-self.padding, :-self.padding].detach().clone()\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        x_curr = x[..., :-self.padding, :-self.padding]\n",
    "        \n",
    "        if not self.training:\n",
    "            residual = x_curr - x_prev\n",
    "            intermediate_stats.append({\n",
    "            'layer': 1,\n",
    "            'residual_norm': residual.norm(dim=1).mean().item(),\n",
    "            'relative_change': (residual.norm(dim=1) / (x_prev.norm(dim=1) + 1e-8)).mean().item(),\n",
    "            'feature_norm': x_curr.norm(dim=1).mean().item()\n",
    "            })\n",
    "\n",
    "        x_prev = x[..., :-self.padding, :-self.padding].detach().clone()\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        x_curr = x[..., :-self.padding, :-self.padding]\n",
    "        \n",
    "        if not self.training:\n",
    "            residual = x_curr - x_prev\n",
    "            intermediate_stats.append({\n",
    "            'layer': 2,\n",
    "            'residual_norm': residual.norm(dim=1).mean().item(),\n",
    "            'relative_change': (residual.norm(dim=1) / (x_prev.norm(dim=1) + 1e-8)).mean().item(),\n",
    "            'feature_norm': x_curr.norm(dim=1).mean().item()\n",
    "            })\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, intermediate_stats\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24d3ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188353\n",
      "Epoch 0: Time 1.66s, Train L2 Loss: 0.1671, Test L2 Loss: 0.1154\n",
      "  Layer 0: avg_residual_norm=3.908563, avg_relative_change=1.272997, avg_feature_norm=2.280371\n",
      "  Layer 1: avg_residual_norm=2.624805, avg_relative_change=1.151839, avg_feature_norm=1.760206\n",
      "  Layer 2: avg_residual_norm=2.057095, avg_relative_change=1.191610, avg_feature_norm=1.316960\n",
      "Epoch 1: Time 1.66s, Train L2 Loss: 0.1073, Test L2 Loss: 0.1066\n",
      "  Layer 0: avg_residual_norm=4.002622, avg_relative_change=1.298255, avg_feature_norm=2.396368\n",
      "  Layer 1: avg_residual_norm=2.808423, avg_relative_change=1.169992, avg_feature_norm=1.970403\n",
      "  Layer 2: avg_residual_norm=2.266054, avg_relative_change=1.183605, avg_feature_norm=1.495954\n",
      "Epoch 2: Time 1.65s, Train L2 Loss: 0.0906, Test L2 Loss: 0.0801\n",
      "  Layer 0: avg_residual_norm=4.145926, avg_relative_change=1.322349, avg_feature_norm=2.519506\n",
      "  Layer 1: avg_residual_norm=2.949790, avg_relative_change=1.167328, avg_feature_norm=2.235728\n",
      "  Layer 2: avg_residual_norm=2.559597, avg_relative_change=1.188949, avg_feature_norm=1.772113\n",
      "Epoch 3: Time 1.65s, Train L2 Loss: 0.0642, Test L2 Loss: 0.0561\n",
      "  Layer 0: avg_residual_norm=4.329775, avg_relative_change=1.355023, avg_feature_norm=2.729129\n",
      "  Layer 1: avg_residual_norm=3.250063, avg_relative_change=1.189705, avg_feature_norm=2.632614\n",
      "  Layer 2: avg_residual_norm=2.961391, avg_relative_change=1.175899, avg_feature_norm=2.095800\n",
      "Epoch 4: Time 1.66s, Train L2 Loss: 0.0496, Test L2 Loss: 0.0452\n",
      "  Layer 0: avg_residual_norm=4.445889, avg_relative_change=1.378600, avg_feature_norm=2.861319\n",
      "  Layer 1: avg_residual_norm=3.436162, avg_relative_change=1.200840, avg_feature_norm=2.846516\n",
      "  Layer 2: avg_residual_norm=3.163695, avg_relative_change=1.154859, avg_feature_norm=2.186244\n",
      "Epoch 5: Time 1.66s, Train L2 Loss: 0.0404, Test L2 Loss: 0.0542\n",
      "  Layer 0: avg_residual_norm=4.536913, avg_relative_change=1.397088, avg_feature_norm=2.972282\n",
      "  Layer 1: avg_residual_norm=3.585272, avg_relative_change=1.207276, avg_feature_norm=3.005812\n",
      "  Layer 2: avg_residual_norm=3.309258, avg_relative_change=1.140040, avg_feature_norm=2.214142\n",
      "Epoch 6: Time 1.66s, Train L2 Loss: 0.0352, Test L2 Loss: 0.0359\n",
      "  Layer 0: avg_residual_norm=4.581883, avg_relative_change=1.403183, avg_feature_norm=3.041323\n",
      "  Layer 1: avg_residual_norm=3.675554, avg_relative_change=1.210147, avg_feature_norm=3.065306\n",
      "  Layer 2: avg_residual_norm=3.389619, avg_relative_change=1.143833, avg_feature_norm=2.228202\n",
      "Epoch 7: Time 1.66s, Train L2 Loss: 0.0312, Test L2 Loss: 0.0318\n",
      "  Layer 0: avg_residual_norm=4.616856, avg_relative_change=1.408155, avg_feature_norm=3.095165\n",
      "  Layer 1: avg_residual_norm=3.735341, avg_relative_change=1.210356, avg_feature_norm=3.090344\n",
      "  Layer 2: avg_residual_norm=3.416547, avg_relative_change=1.142697, avg_feature_norm=2.178604\n",
      "Epoch 8: Time 1.66s, Train L2 Loss: 0.0280, Test L2 Loss: 0.0369\n",
      "  Layer 0: avg_residual_norm=4.655960, avg_relative_change=1.413972, avg_feature_norm=3.146198\n",
      "  Layer 1: avg_residual_norm=3.794251, avg_relative_change=1.211027, avg_feature_norm=3.122172\n",
      "  Layer 2: avg_residual_norm=3.445623, avg_relative_change=1.141006, avg_feature_norm=2.141028\n",
      "Epoch 9: Time 1.66s, Train L2 Loss: 0.0269, Test L2 Loss: 0.0299\n",
      "  Layer 0: avg_residual_norm=4.675569, avg_relative_change=1.416059, avg_feature_norm=3.167716\n",
      "  Layer 1: avg_residual_norm=3.831987, avg_relative_change=1.215231, avg_feature_norm=3.122659\n",
      "  Layer 2: avg_residual_norm=3.451525, avg_relative_change=1.142818, avg_feature_norm=2.096061\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "model = FNO2d(modes, modes, width).cuda()\n",
    "print(count_params(model))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, _ = model(x)\n",
    "        out = out.reshape(batch_size, s, s)\n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "\n",
    "    # accumulator for intermediate stats across test batches\n",
    "    stats_acc = {}  # layer -> {'residual_norm': [...], 'relative_change': [...], 'feature_norm': [...]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            out, intermediate_stats = model(x)\n",
    "            out = out.reshape(out.shape[0], s, s)  # use actual batch size\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(out.shape[0],-1), y.view(out.shape[0],-1)).item()\n",
    "\n",
    "            # collect intermediate stats (each entry already averaged over batch/spatial dims in forward)\n",
    "            for st in intermediate_stats:\n",
    "                layer = st['layer']\n",
    "                stats_acc.setdefault(layer, {'residual_norm': [], 'relative_change': [], 'feature_norm': []})\n",
    "                stats_acc[layer]['residual_norm'].append(st['residual_norm'])\n",
    "                stats_acc[layer]['relative_change'].append(st['relative_change'])\n",
    "                stats_acc[layer]['feature_norm'].append(st['feature_norm'])\n",
    "\n",
    "    train_l2 /= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(f'Epoch {ep}: Time {t2-t1:.2f}s, Train L2 Loss: {train_l2:.4f}, Test L2 Loss: {test_l2:.4f}')\n",
    "\n",
    "    # print batch-averaged intermediate stats per layer\n",
    "    if stats_acc:\n",
    "        for layer in sorted(stats_acc.keys()):\n",
    "            avg_res = np.mean(stats_acc[layer]['residual_norm'])\n",
    "            avg_rel = np.mean(stats_acc[layer]['relative_change'])\n",
    "            avg_feat = np.mean(stats_acc[layer]['feature_norm'])\n",
    "            print(f'  Layer {layer}: avg_residual_norm={avg_res:.6f}, avg_relative_change={avg_rel:.6f}, avg_feature_norm={avg_feat:.6f}')\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a9d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
